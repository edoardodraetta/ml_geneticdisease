{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb42104-5d7c-486a-a0dc-bdcf5e23df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 85)\n",
    "sns.set_theme(context=\"paper\", font_scale=1.5, style=\"ticks\", rc={\"axes.grid\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b787d-8d59-4ecb-8810-7a66a82974c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Why is PCA to 100% Explained Variance so different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6457838-7636-41a6-91ce-dec2a75df921",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dc902c-82ad-4c0c-8f6f-391d33db3d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data from CSV (NEW DATA, NOT SCALED)\n",
    "df = pd.read_csv(\"../data/abnormal_writeout_noscale.data.csv\", index_col=0)\n",
    "\n",
    "# trascurare da ACC a UVM\n",
    "start_drop = df.columns.get_loc(\"ACC\")\n",
    "end_drop = df.columns.get_loc(\"UVM\")\n",
    "cols = np.arange(start_drop, end_drop + 1)\n",
    "df.drop(df.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "# trascurare alcune colonne\n",
    "df.drop(\"TTT_freq\", axis=1, inplace=True)\n",
    "df.drop(\"oldest_phylostratum_factor\", axis=1, inplace=True)\n",
    "df.drop(\"gc_cds\", axis=1, inplace=True) # ! New\n",
    "\n",
    "# Drop NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Sort features\n",
    "resp = df[\"response\"]\n",
    "occ = df[\"occ_total_sum\"]\n",
    "age = df[\"oldest_phylostratum\"]\n",
    "conf = df.drop(labels=[\"response\", \"occ_total_sum\", \"oldest_phylostratum\"], axis=1)\n",
    "\n",
    "# Collect Features and Labels\n",
    "features_df = pd.DataFrame()\n",
    "features_df[\"occ_total_sum\"] = occ\n",
    "features_df[\"oldest_phylostratum\"] = age\n",
    "features_df = pd.concat([features_df, conf], axis=1)\n",
    "\n",
    "X = features_df.to_numpy()\n",
    "y = df[\"response\"].to_numpy()\n",
    "\n",
    "features_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb2f43d-d9b3-4a50-afa8-0c03608c42aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Custom PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f1b67-bcb1-46cf-9a98-7663ab9f9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Columns of confounder variables (highly colinear)\n",
    "conf_index = 2\n",
    "conf_cols = np.arange(2, X.shape[1])\n",
    "\n",
    "\n",
    "class ConfounderPCA(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" \n",
    "    Custom PCA transformer for this dataset.\n",
    "    Applies PCA only to the many collinear confounder \n",
    "    variables.\n",
    "    \n",
    "    cols - columns to which PCA will be applied.\n",
    "    \n",
    "    n_components - same as with the \"vanilla\" PCA. \n",
    "        If 0 < n_components < 1, select the number of \n",
    "        components such that the amount of variance that \n",
    "        needs to be explained is greater than the \n",
    "        percentage specified by n_components.\n",
    "        \n",
    "    apply_PCA - if false, simply returns the untransformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cols, n_components=None, apply_PCA=True):\n",
    "        self.n_components = n_components\n",
    "        self.apply_PCA = apply_PCA\n",
    "        self.cols = cols\n",
    "        if self.apply_PCA:\n",
    "            self.pca = PCA(n_components=self.n_components)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.apply_PCA:\n",
    "            self.pca.fit(X[:, self.cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.apply_PCA:\n",
    "            X_pca = self.pca.transform(X[:, self.cols])\n",
    "            return np.c_[X[:, :2], X_pca]\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "\n",
    "sns.heatmap(\n",
    "    pd.DataFrame(\n",
    "        ConfounderPCA(cols=np.arange(2, X.shape[1])).fit_transform(StandardScaler().fit_transform(X))\n",
    "    ).corr()\n",
    ")\n",
    "plt.title(\"Correlation Matrix after PCA\")\n",
    "plt.show()\n",
    "\n",
    "print(X.shape[1], \"total features.\")\n",
    "print(\"Confounder columns start from index\", conf_index, \"of feature matrix.\")\n",
    "print(\"Non-counfounders:\", features_df.iloc[:, 0:conf_index].columns.tolist())\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17672886-c9c5-48f8-9e7d-cd5691ea808d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb83db97-8e2c-407e-a5b0-7bd1b91f9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "for i in [0.9, 0.95, 0.99, 0.999, None]:\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\", ConfounderPCA(cols=np.arange(2, X.shape[1]), n_components=i)),\n",
    "        (\"lr\", LogisticRegression(max_iter=2000,)),\n",
    "    ])\n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    if i != None:\n",
    "        imp = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=int(i*5), scoring='roc_auc', n_jobs=-1)\n",
    "        imp_df = pd.DataFrame(imp.importances_mean, columns=[\"Score\"])\n",
    "        imp_scores = imp_df.sort_values(by=\"Score\", key=abs, ascending=False).to_numpy()\n",
    "        sns.lineplot(x=np.arange(83), y=imp_scores.flatten(), label=f\"{i*100}% EV\") \n",
    "    else:\n",
    "        imp = permutation_importance(clf, X_train, y_train, n_repeats=10, random_state=42, scoring='roc_auc', n_jobs=-1)\n",
    "        imp_df = pd.DataFrame(imp.importances_mean, columns=[\"Score\"])\n",
    "        imp_scores = imp_df.sort_values(by=\"Score\", key=abs, ascending=False).to_numpy()\n",
    "        sns.lineplot(x=np.arange(83), y=imp_scores.flatten(), label=\"100% EV\") \n",
    "\n",
    "    plt.title(\"LR Feature Importance by Absolute Effect on ROC-AUC Score\")\n",
    "    plt.ylabel(\"ROC-AUC Score Impact\")\n",
    "    plt.xlabel(\"Feature Rank\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71bfaa-1287-4d26-9967-a9b6e016285b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b7fb2-2862-463d-8470-20fda1bd4555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_venv]",
   "language": "python",
   "name": "conda-env-ml_venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
