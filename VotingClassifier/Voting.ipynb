{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5794e2f-f85e-43bc-8eea-58fb6488eb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7461de-9499-4a18-8166-ef37899428e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Voting Classifier\n",
    "\n",
    "A voting classifier aggregates the predictions from an ensemble of models and predict the class with the most votes (hard voting) or highest average probability (soft voting). \n",
    "\n",
    "> Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. _Gèron, Ch 7_\n",
    "\n",
    "A voting classifier performs best when the member classifiers make independent predictions. \n",
    "\n",
    "> Ensemble methods work best when the predictors are as independent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy. _Gèron, Ch 7_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b78c75-65d2-490f-9638-e09f04797008",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"../images/voting_classifier.png\" alt=\"Voting Classifier (Gèron)\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc06492b-abfd-4dee-9a06-fad4da9b0f2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (14536, 83) (3634, 83)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/abnormal_writeout_noscale.data.csv\")\n",
    "\n",
    "# trascurare da ACC a UVM\n",
    "start_drop = df.columns.get_loc(\"ACC\")\n",
    "end_drop = df.columns.get_loc(\"UVM\")\n",
    "cols = np.arange(start_drop, end_drop + 1)\n",
    "df.drop(df.columns[cols], axis=1, inplace=True)\n",
    "\n",
    "# trascurare alcune colonne\n",
    "df.drop(\"TTT_freq\", axis=1, inplace=True)\n",
    "df.drop(\"oldest_phylostratum_factor\", axis=1, inplace=True)\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "# Drop NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Sort features\n",
    "resp = df[\"response\"].to_numpy()\n",
    "occ = df[\"occ_total_sum\"]\n",
    "age = df[\"oldest_phylostratum\"].to_numpy()\n",
    "conf = df.drop(labels=[\"response\", \"occ_total_sum\", \"oldest_phylostratum\"], axis=1).to_numpy()\n",
    "\n",
    "features_df = \n",
    "X = np.c_[occ, age, conf]\n",
    "Y = resp\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(\"Feature matrix shape:\", X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b48b7679-5a7f-4d5b-a03b-63fca4a13af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>0.657258</td>\n",
       "      <td>0.612230</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.561429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216855</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041809</td>\n",
       "      <td>0.809254</td>\n",
       "      <td>0.706453</td>\n",
       "      <td>6.798234</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.026495</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.017663</td>\n",
       "      <td>0.044837</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.033967</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>0.023777</td>\n",
       "      <td>0.030571</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>873.0</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.086769</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.657839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007148</td>\n",
       "      <td>0.828752</td>\n",
       "      <td>1.097018</td>\n",
       "      <td>0.061963</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.043350</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.024110</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.035591</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.027555</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>0.572344</td>\n",
       "      <td>0.479295</td>\n",
       "      <td>0.611722</td>\n",
       "      <td>0.851369</td>\n",
       "      <td>0.354628</td>\n",
       "      <td>0.618954</td>\n",
       "      <td>0.754579</td>\n",
       "      <td>0.03022</td>\n",
       "      <td>0.086996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040463</td>\n",
       "      <td>1.249600</td>\n",
       "      <td>1.354306</td>\n",
       "      <td>6.081620</td>\n",
       "      <td>0.028404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.868383</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.034644</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.021536</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.006554</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1       2         3         4         5         6         7   \\\n",
       "0  33.0  12.0  1488.0  0.657258  0.612230  0.758065  0.561429  1.000000   \n",
       "1  28.0   1.0   873.0  0.422680  0.086769  0.195876  0.657839  0.000000   \n",
       "2  36.0   1.0  1092.0  0.572344  0.479295  0.611722  0.851369  0.354628   \n",
       "\n",
       "         8         9        10        11   12        13        14        15  \\\n",
       "0  0.216855  0.661290  1.00000  0.198925  0.0  0.041809  0.809254  0.706453   \n",
       "1  0.000000  0.000000  0.00000  0.000000  1.0 -0.007148  0.828752  1.097018   \n",
       "2  0.618954  0.754579  0.03022  0.086996  0.0  0.040463  1.249600  1.354306   \n",
       "\n",
       "         16        17   18        19        20        21        22        23  \\\n",
       "0  6.798234  0.040516  0.0  0.000000  0.004755  0.008152  0.007473  0.002717   \n",
       "1  0.061963  0.002809  0.0  2.043350  0.025258  0.019518  0.021814  0.024110   \n",
       "2  6.081620  0.028404  0.0  0.868383  0.018727  0.012172  0.023408  0.003745   \n",
       "\n",
       "         24        25        26        27        28        29        30  \\\n",
       "0  0.011549  0.026495  0.010870  0.008152  0.010190  0.028533  0.019701   \n",
       "1  0.025258  0.018370  0.003444  0.012629  0.035591  0.009185  0.016073   \n",
       "2  0.017790  0.024345  0.007491  0.014981  0.024345  0.020599  0.025281   \n",
       "\n",
       "         31        32        33        34        35        36        37  \\\n",
       "0  0.009511  0.000679  0.006114  0.010870  0.002038  0.009511  0.019022   \n",
       "1  0.006889  0.016073  0.017222  0.010333  0.033295  0.019518  0.011481   \n",
       "2  0.011236  0.003745  0.013109  0.019663  0.004682  0.017790  0.016854   \n",
       "\n",
       "         38        39        40        41        42        43        44  \\\n",
       "0  0.028533  0.007473  0.027174  0.031250  0.025136  0.029891  0.015625   \n",
       "1  0.020666  0.022962  0.017222  0.008037  0.002296  0.021814  0.003444   \n",
       "2  0.029963  0.017790  0.034644  0.022472  0.010300  0.028090  0.005618   \n",
       "\n",
       "         45        46        47        48        49        50        51  \\\n",
       "0  0.027174  0.019701  0.009511  0.007473  0.017663  0.044837  0.013587   \n",
       "1  0.001148  0.004592  0.002296  0.008037  0.019518  0.022962  0.019518   \n",
       "2  0.010300  0.014045  0.003745  0.015918  0.015918  0.033708  0.011236   \n",
       "\n",
       "         52        53        54        55        56        57        58  \\\n",
       "0  0.008832  0.021739  0.031250  0.008152  0.016984  0.033967  0.027853   \n",
       "1  0.033295  0.013777  0.019518  0.011481  0.014925  0.006889  0.000000   \n",
       "2  0.014981  0.022472  0.026217  0.009363  0.015918  0.031835  0.007491   \n",
       "\n",
       "         59        60        61        62        63        64        65  \\\n",
       "0  0.034647  0.023777  0.030571  0.029212  0.013587  0.000679  0.012908   \n",
       "1  0.012629  0.018370  0.011481  0.017222  0.018370  0.005741  0.008037   \n",
       "2  0.025281  0.028090  0.029026  0.021536  0.013109  0.008427  0.010300   \n",
       "\n",
       "         66        67        68        69        70        71        72  \\\n",
       "0  0.027174  0.003397  0.000000  0.008152  0.000000  0.001359  0.008832   \n",
       "1  0.012629  0.012629  0.012629  0.014925  0.006889  0.017222  0.017222   \n",
       "2  0.016854  0.003745  0.006554  0.012172  0.005618  0.008427  0.014981   \n",
       "\n",
       "         73        74        75        76        77        78        79  \\\n",
       "0  0.021739  0.009511  0.010190  0.020380  0.027174  0.029212  0.010870   \n",
       "1  0.016073  0.005741  0.022962  0.020666  0.012629  0.027555  0.011481   \n",
       "2  0.016854  0.009363  0.008427  0.014981  0.019663  0.029026  0.010300   \n",
       "\n",
       "         80        81        82  \n",
       "0  0.000679  0.013587  0.005435  \n",
       "1  0.021814  0.017222  0.026406  \n",
       "2  0.004682  0.010300  0.004682  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb9bdba-043b-41bb-be70-68435318d363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>cds_length</th>\n",
       "      <th>occ_total_sum</th>\n",
       "      <th>oldest_phylostratum</th>\n",
       "      <th>gc_cds</th>\n",
       "      <th>dnase_gene</th>\n",
       "      <th>dnase_cds</th>\n",
       "      <th>H3k4me1_gene</th>\n",
       "      <th>H3k4me3_gene</th>\n",
       "      <th>H3k27ac_gene</th>\n",
       "      <th>H3k4me1_cds</th>\n",
       "      <th>H3k4me3_cds</th>\n",
       "      <th>H3k27ac_cds</th>\n",
       "      <th>lamin_gene</th>\n",
       "      <th>repli_gene</th>\n",
       "      <th>nsome_gene</th>\n",
       "      <th>nsome_cds</th>\n",
       "      <th>transcription_gene</th>\n",
       "      <th>repeat_gene</th>\n",
       "      <th>repeat_cds</th>\n",
       "      <th>recomb_gene</th>\n",
       "      <th>AAA_freq</th>\n",
       "      <th>AAC_freq</th>\n",
       "      <th>AAG_freq</th>\n",
       "      <th>AAT_freq</th>\n",
       "      <th>ACA_freq</th>\n",
       "      <th>ACC_freq</th>\n",
       "      <th>ACG_freq</th>\n",
       "      <th>ACT_freq</th>\n",
       "      <th>AGA_freq</th>\n",
       "      <th>AGC_freq</th>\n",
       "      <th>AGG_freq</th>\n",
       "      <th>AGT_freq</th>\n",
       "      <th>ATA_freq</th>\n",
       "      <th>ATC_freq</th>\n",
       "      <th>ATG_freq</th>\n",
       "      <th>ATT_freq</th>\n",
       "      <th>CAA_freq</th>\n",
       "      <th>CAC_freq</th>\n",
       "      <th>CAG_freq</th>\n",
       "      <th>CAT_freq</th>\n",
       "      <th>CCA_freq</th>\n",
       "      <th>CCC_freq</th>\n",
       "      <th>CCG_freq</th>\n",
       "      <th>CCT_freq</th>\n",
       "      <th>CGA_freq</th>\n",
       "      <th>CGC_freq</th>\n",
       "      <th>CGG_freq</th>\n",
       "      <th>CGT_freq</th>\n",
       "      <th>CTA_freq</th>\n",
       "      <th>CTC_freq</th>\n",
       "      <th>CTG_freq</th>\n",
       "      <th>CTT_freq</th>\n",
       "      <th>GAA_freq</th>\n",
       "      <th>GAC_freq</th>\n",
       "      <th>GAG_freq</th>\n",
       "      <th>GAT_freq</th>\n",
       "      <th>GCA_freq</th>\n",
       "      <th>GCC_freq</th>\n",
       "      <th>GCG_freq</th>\n",
       "      <th>GCT_freq</th>\n",
       "      <th>GGA_freq</th>\n",
       "      <th>GGC_freq</th>\n",
       "      <th>GGG_freq</th>\n",
       "      <th>GGT_freq</th>\n",
       "      <th>GTA_freq</th>\n",
       "      <th>GTC_freq</th>\n",
       "      <th>GTG_freq</th>\n",
       "      <th>GTT_freq</th>\n",
       "      <th>TAA_freq</th>\n",
       "      <th>TAC_freq</th>\n",
       "      <th>TAG_freq</th>\n",
       "      <th>TAT_freq</th>\n",
       "      <th>TCA_freq</th>\n",
       "      <th>TCC_freq</th>\n",
       "      <th>TCG_freq</th>\n",
       "      <th>TCT_freq</th>\n",
       "      <th>TGA_freq</th>\n",
       "      <th>TGC_freq</th>\n",
       "      <th>TGG_freq</th>\n",
       "      <th>TGT_freq</th>\n",
       "      <th>TTA_freq</th>\n",
       "      <th>TTC_freq</th>\n",
       "      <th>TTG_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1488</td>\n",
       "      <td>33</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.657258</td>\n",
       "      <td>0.612230</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.561429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216855</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041809</td>\n",
       "      <td>0.809254</td>\n",
       "      <td>0.706453</td>\n",
       "      <td>6.798234</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.026495</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.017663</td>\n",
       "      <td>0.044837</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.033967</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>0.023777</td>\n",
       "      <td>0.030571</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>873</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.086769</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.657839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007148</td>\n",
       "      <td>0.828752</td>\n",
       "      <td>1.097018</td>\n",
       "      <td>0.061963</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.043350</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.024110</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.035591</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.027555</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1092</td>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572344</td>\n",
       "      <td>0.479295</td>\n",
       "      <td>0.611722</td>\n",
       "      <td>0.851369</td>\n",
       "      <td>0.354628</td>\n",
       "      <td>0.618954</td>\n",
       "      <td>0.754579</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.086996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040463</td>\n",
       "      <td>1.249600</td>\n",
       "      <td>1.354306</td>\n",
       "      <td>6.081620</td>\n",
       "      <td>0.028404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.868383</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.034644</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.021536</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.006554</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2800</td>\n",
       "      <td>126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>0.554023</td>\n",
       "      <td>0.052420</td>\n",
       "      <td>0.278492</td>\n",
       "      <td>0.270357</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.151429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.022495</td>\n",
       "      <td>0.921420</td>\n",
       "      <td>1.382249</td>\n",
       "      <td>2.254471</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.143060</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.022415</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.015546</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.021330</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.027477</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.023861</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>0.019161</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.010846</td>\n",
       "      <td>0.025307</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.022415</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>0.020607</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.013738</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.016631</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.018077</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.009038</td>\n",
       "      <td>0.031092</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.015907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1484</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.401617</td>\n",
       "      <td>0.143843</td>\n",
       "      <td>0.030997</td>\n",
       "      <td>0.400789</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>0.457949</td>\n",
       "      <td>0.708221</td>\n",
       "      <td>0.030997</td>\n",
       "      <td>0.659704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>0.960747</td>\n",
       "      <td>1.196871</td>\n",
       "      <td>1.080241</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.217000</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.025412</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.032280</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.026099</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.019231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response  cds_length  occ_total_sum  oldest_phylostratum    gc_cds  \\\n",
       "0         0        1488             33                 12.0  0.657258   \n",
       "1         0         873             28                  1.0  0.422680   \n",
       "2         1        1092             36                  1.0  0.572344   \n",
       "3         0        2800            126                  1.0  0.460000   \n",
       "4         1        1484             55                  1.0  0.401617   \n",
       "\n",
       "   dnase_gene  dnase_cds  H3k4me1_gene  H3k4me3_gene  H3k27ac_gene  \\\n",
       "0    0.612230   0.758065      0.561429      1.000000      0.216855   \n",
       "1    0.086769   0.195876      0.657839      0.000000      0.000000   \n",
       "2    0.479295   0.611722      0.851369      0.354628      0.618954   \n",
       "3    0.171524   0.280357      0.554023      0.052420      0.278492   \n",
       "4    0.143843   0.030997      0.400789      0.106455      0.457949   \n",
       "\n",
       "   H3k4me1_cds  H3k4me3_cds  H3k27ac_cds  lamin_gene  repli_gene  nsome_gene  \\\n",
       "0     0.661290     1.000000     0.198925         0.0    0.041809    0.809254   \n",
       "1     0.000000     0.000000     0.000000         1.0   -0.007148    0.828752   \n",
       "2     0.754579     0.030220     0.086996         0.0    0.040463    1.249600   \n",
       "3     0.270357     0.021429     0.151429         0.0   -0.022495    0.921420   \n",
       "4     0.708221     0.030997     0.659704         0.0   -0.000387    0.960747   \n",
       "\n",
       "   nsome_cds  transcription_gene  repeat_gene  repeat_cds  recomb_gene  \\\n",
       "0   0.706453            6.798234     0.040516         0.0     0.000000   \n",
       "1   1.097018            0.061963     0.002809         0.0     2.043350   \n",
       "2   1.354306            6.081620     0.028404         0.0     0.868383   \n",
       "3   1.382249            2.254471     0.014520         0.0     1.143060   \n",
       "4   1.196871            1.080241     0.009545         0.0     4.217000   \n",
       "\n",
       "   AAA_freq  AAC_freq  AAG_freq  AAT_freq  ACA_freq  ACC_freq  ACG_freq  \\\n",
       "0  0.004755  0.008152  0.007473  0.002717  0.011549  0.026495  0.010870   \n",
       "1  0.025258  0.019518  0.021814  0.024110  0.025258  0.018370  0.003444   \n",
       "2  0.018727  0.012172  0.023408  0.003745  0.017790  0.024345  0.007491   \n",
       "3  0.022054  0.014823  0.022415  0.024946  0.022054  0.014100  0.006146   \n",
       "4  0.039835  0.015797  0.030220  0.025412  0.024038  0.012363  0.002747   \n",
       "\n",
       "   ACT_freq  AGA_freq  AGC_freq  AGG_freq  AGT_freq  ATA_freq  ATC_freq  \\\n",
       "0  0.008152  0.010190  0.028533  0.019701  0.009511  0.000679  0.006114   \n",
       "1  0.012629  0.035591  0.009185  0.016073  0.006889  0.016073  0.017222   \n",
       "2  0.014981  0.024345  0.020599  0.025281  0.011236  0.003745  0.013109   \n",
       "3  0.015546  0.024946  0.016992  0.012292  0.015907  0.013377  0.021330   \n",
       "4  0.019918  0.048077  0.006868  0.015797  0.009615  0.020604  0.009615   \n",
       "\n",
       "   ATG_freq  ATT_freq  CAA_freq  CAC_freq  CAG_freq  CAT_freq  CCA_freq  \\\n",
       "0  0.010870  0.002038  0.009511  0.019022  0.028533  0.007473  0.027174   \n",
       "1  0.010333  0.033295  0.019518  0.011481  0.020666  0.022962  0.017222   \n",
       "2  0.019663  0.004682  0.017790  0.016854  0.029963  0.017790  0.034644   \n",
       "3  0.026392  0.017715  0.026392  0.011931  0.027477  0.017354  0.023861   \n",
       "4  0.032280  0.023352  0.019918  0.012363  0.021978  0.015797  0.015110   \n",
       "\n",
       "   CCC_freq  CCG_freq  CCT_freq  CGA_freq  CGC_freq  CGG_freq  CGT_freq  \\\n",
       "0  0.031250  0.025136  0.029891  0.015625  0.027174  0.019701  0.009511   \n",
       "1  0.008037  0.002296  0.021814  0.003444  0.001148  0.004592  0.002296   \n",
       "2  0.022472  0.010300  0.028090  0.005618  0.010300  0.014045  0.003745   \n",
       "3  0.016992  0.006508  0.019161  0.005785  0.003977  0.007954  0.003977   \n",
       "4  0.003434  0.004121  0.013049  0.005495  0.001374  0.002060  0.002060   \n",
       "\n",
       "   CTA_freq  CTC_freq  CTG_freq  CTT_freq  GAA_freq  GAC_freq  GAG_freq  \\\n",
       "0  0.007473  0.017663  0.044837  0.013587  0.008832  0.021739  0.031250   \n",
       "1  0.008037  0.019518  0.022962  0.019518  0.033295  0.013777  0.019518   \n",
       "2  0.015918  0.015918  0.033708  0.011236  0.014981  0.022472  0.026217   \n",
       "3  0.006146  0.010846  0.025307  0.015907  0.022415  0.022777  0.016269   \n",
       "4  0.013736  0.014423  0.014423  0.013736  0.034341  0.017857  0.024725   \n",
       "\n",
       "   GAT_freq  GCA_freq  GCC_freq  GCG_freq  GCT_freq  GGA_freq  GGC_freq  \\\n",
       "0  0.008152  0.016984  0.033967  0.027853  0.034647  0.023777  0.030571   \n",
       "1  0.011481  0.014925  0.006889  0.000000  0.012629  0.018370  0.011481   \n",
       "2  0.009363  0.015918  0.031835  0.007491  0.025281  0.028090  0.029026   \n",
       "3  0.018800  0.015184  0.016992  0.004700  0.014461  0.017354  0.010484   \n",
       "4  0.024725  0.016484  0.006868  0.002747  0.006181  0.022665  0.013049   \n",
       "\n",
       "   GGG_freq  GGT_freq  GTA_freq  GTC_freq  GTG_freq  GTT_freq  TAA_freq  \\\n",
       "0  0.029212  0.013587  0.000679  0.012908  0.027174  0.003397  0.000000   \n",
       "1  0.017222  0.018370  0.005741  0.008037  0.012629  0.012629  0.012629   \n",
       "2  0.021536  0.013109  0.008427  0.010300  0.016854  0.003745  0.006554   \n",
       "3  0.010123  0.011931  0.009400  0.007231  0.020607  0.011931  0.013738   \n",
       "4  0.010302  0.008242  0.009615  0.004808  0.013736  0.011676  0.018544   \n",
       "\n",
       "   TAC_freq  TAG_freq  TAT_freq  TCA_freq  TCC_freq  TCG_freq  TCT_freq  \\\n",
       "0  0.008152  0.000000  0.001359  0.008832  0.021739  0.009511  0.010190   \n",
       "1  0.014925  0.006889  0.017222  0.017222  0.016073  0.005741  0.022962   \n",
       "2  0.012172  0.005618  0.008427  0.014981  0.016854  0.009363  0.008427   \n",
       "3  0.008315  0.006146  0.016631  0.022054  0.018077  0.004700  0.009038   \n",
       "4  0.012363  0.008242  0.019231  0.015110  0.012363  0.002060  0.015797   \n",
       "\n",
       "   TGA_freq  TGC_freq  TGG_freq  TGT_freq  TTA_freq  TTC_freq  TTG_freq  \n",
       "0  0.020380  0.027174  0.029212  0.010870  0.000679  0.013587  0.005435  \n",
       "1  0.020666  0.012629  0.027555  0.011481  0.021814  0.017222  0.026406  \n",
       "2  0.014981  0.019663  0.029026  0.010300  0.004682  0.010300  0.004682  \n",
       "3  0.031092  0.019523  0.019523  0.016992  0.016269  0.014100  0.015907  \n",
       "4  0.024038  0.010989  0.026099  0.018544  0.014423  0.015797  0.019231  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b3416f7-f7fc-4656-ba2e-2f0010720ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "conf_cols = np.arange(:2, X.shape[1])  # Columns of confounder variables (highly colinear)\n",
    "\n",
    "\n",
    "class ConfounderPCA(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Custom PCA transformer for this dataset\"\"\"\n",
    "\n",
    "    def __init__(self, explained_variance=0.95, apply_PCA=True):\n",
    "        self.apply_PCA = apply_PCA\n",
    "        self.explained_variance = explained_variance\n",
    "        if self.apply_PCA:\n",
    "            self.pca = PCA(n_components=self.explained_variance)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.apply_PCA:\n",
    "            self.pca.fit(X[:, conf_cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.apply_PCA:\n",
    "            X_conf_pca = self.pca.transform(X[:, conf_cols])\n",
    "            return np.c_[X[:, :2], X_conf_pca]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becb9f2b-9634-49c0-8631-aba21fb2ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, make_scorer, precision_recall_curve\n",
    "\n",
    "\n",
    "def auprc(y_true, y_scores, **kwargs):\n",
    "    \"\"\" Remember to use make_scorer(auprc, needs_proba=True,) \"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "    # result is sum of the areas under each curve\n",
    "    return auc(thresholds, precisions[:-1]) + auc(thresholds, recalls[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c298a23-80f6-4876-beaa-18989f67a6d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1 - [Spearman Correlation Coefficient](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html)\n",
    "\n",
    "The _spearman correlation coefficient_ is defined as the pearson correlation coefficient between the rank variables:\n",
    "\n",
    "$$\n",
    "r_s = \\rho_{R(X), R(Y)} = \\frac{Cov(R(X), R(Y))}{\\sigma_R(X), \\sigma_R(Y)}.\n",
    "$$\n",
    "\n",
    "\n",
    "If all $n$ ranks are distinct, it can be computed using\n",
    "\n",
    "$$\n",
    "r_s = 1 - \\frac{6 \\sum d_i^2 }{n(n^2-1)} \\\\ \\\\\n",
    "d_i = R(X_i) - R(Y_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9b71bf1-6019-4b80-b96e-7e2fa6c533f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "rf_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), # !! Absolutely necessary before PCA\n",
    "    (\"pca\", ConfounderPCA()), \n",
    "    (\"rf\", BalancedRandomForestClassifier(\n",
    "        max_depth=10,\n",
    "        max_features=2,\n",
    "        min_samples_leaf=2, \n",
    "        min_samples_split=20, \n",
    "        n_estimators=500,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Improves convergence\n",
    "    # (\"pca\", ConfounderPCA()), \n",
    "    (\"lr\", LogisticRegression(max_iter=500, class_weight='balanced')),\n",
    "])\n",
    "\n",
    "gnb_clf = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    # (\"pca\", ConfounderPCA()), \n",
    "    (\"gnb\", GaussianNB()),\n",
    "])\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    # (\"pca\", ConfounderPCA()),\n",
    "    (\"svc\", SVC(\n",
    "        C=1.0, shrinking=False,\n",
    "        class_weight='balanced',\n",
    "        kernel=\"rbf\", \n",
    "        probability=True,\n",
    "        random_state=0,\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc7e3e3-9c3a-4f31-b3f4-92b0f5095a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svc',\n",
       "                 SVC(class_weight='balanced', probability=True, random_state=0,\n",
       "                     shrinking=False))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(X_train, y_train)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "gnb_clf.fit(X_train, y_train)\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ba8ac83-8612-4184-82d0-fc235d4872f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "\n",
    "pred_df[\"LR_pred\"] = lr_clf.predict(X_test)\n",
    "pred_df[\"RF_pred\"] = rf_clf.predict(X_test)\n",
    "pred_df[\"GNB_pred\"] = gnb_clf.predict(X_test)\n",
    "pred_df[\"SVM_pred\"] = svm_clf.predict(X_test)\n",
    "\n",
    "proba_df = pd.DataFrame()\n",
    "\n",
    "proba_df[\"LR_proba\"] = lr_clf.predict_proba(X_test)[:, 0]\n",
    "proba_df[\"RF_proba\"] = rf_clf.predict_proba(X_test)[:, 0]\n",
    "proba_df[\"GNB_proba\"] = gnb_clf.predict_proba(X_test)[:, 0]\n",
    "proba_df[\"SVM_proba\"] = svm_clf.predict_proba(X_test)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c6a97e4-177f-4570-ae23-bf7d7bdb8e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Build the plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n",
    "sns.set(font_scale=1.4)\n",
    "\n",
    "# Add labels to the plot\n",
    "model_names = [\"lr\", \"rf\", \"gnb\", \"svm\"]\n",
    "tick_marks = np.arange(len(model_names)) + 0.1\n",
    "tick_marks2 = tick_marks + 0.4\n",
    "\n",
    "# Plot 1\n",
    "sns.heatmap(spearmanr(pred_df).correlation, annot=True, annot_kws={\"size\": 15}, cmap=plt.cm.Greens, linewidths=0.2, ax=ax[0])\n",
    "ax[0].set_xticks(tick_marks)\n",
    "ax[0].set_xticklabels(model_names)\n",
    "ax[0].tick_params(axis=\"x\", labelrotation=25)\n",
    "ax[0].set_yticks(tick_marks2)\n",
    "ax[0].set_yticklabels(model_names)\n",
    "ax[0].tick_params(axis=\"y\", labelrotation=0)\n",
    "ax[0].title.set_text(\"Spearman Correlation Matrix Between Model Predictions\")\n",
    "\n",
    "# Plot 2\n",
    "sns.heatmap(spearmanr(proba_df).correlation, annot=True, annot_kws={\"size\": 15}, cmap=plt.cm.Greens, linewidths=0.2, ax=ax[1])\n",
    "ax[1].set_xticks(tick_marks)\n",
    "ax[1].set_xticklabels(model_names)\n",
    "ax[1].tick_params(axis=\"x\", labelrotation=25)\n",
    "ax[1].set_yticks(tick_marks2)\n",
    "ax[1].set_yticklabels(model_names)\n",
    "ax[1].tick_params(axis=\"y\", labelrotation=0)\n",
    "ax[1].title.set_text(\"Spearman Correlation Matrix Between Model Probabilities\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b1cab5-13dd-447c-a438-98bb255ac364",
   "metadata": {},
   "source": [
    "**Figure: Spearman Correlation Matrices for member model predictions and probabilities.** Because model predictions can diverge to 0 and 1 despite similar probabilities, the correlations between predictions are always lower than the correlations between probabilities. By the figure on the left, it would seem that the gaussian naive bayes classifier is the most independent from the other classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b38612b-4751-452a-85c4-08e600af3849",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2 - Feature Importance\n",
    "\n",
    "If each of the member models makes predictions using different features, that would be a good indicator that the voting method should perform better \n",
    "\n",
    "[Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)  \n",
    "[Random Forest Feature Importance](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7626a77-c7d7-473c-b8d9-dde91062a8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('lr',\n",
       "                 LogisticRegression(class_weight='balanced', max_iter=500))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    (\"lr\", LogisticRegression(max_iter=500, class_weight='balanced')),\n",
    "])\n",
    "\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21d23ed7-ce03-4bc6-8241-363192cca952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cds_length</th>\n",
       "      <th>AGT_freq</th>\n",
       "      <th>GTA_freq</th>\n",
       "      <th>repli_gene</th>\n",
       "      <th>GCG_freq</th>\n",
       "      <th>TCT_freq</th>\n",
       "      <th>CGA_freq</th>\n",
       "      <th>TGT_freq</th>\n",
       "      <th>GGT_freq</th>\n",
       "      <th>TTA_freq</th>\n",
       "      <th>GTC_freq</th>\n",
       "      <th>nsome_gene</th>\n",
       "      <th>lamin_gene</th>\n",
       "      <th>dnase_gene</th>\n",
       "      <th>TTG_freq</th>\n",
       "      <th>TGG_freq</th>\n",
       "      <th>GGG_freq</th>\n",
       "      <th>AAG_freq</th>\n",
       "      <th>occ_total_sum</th>\n",
       "      <th>H3k4me1_cds</th>\n",
       "      <th>nsome_cds</th>\n",
       "      <th>H3k4me3_gene</th>\n",
       "      <th>ATC_freq</th>\n",
       "      <th>H3k4me3_cds</th>\n",
       "      <th>recomb_gene</th>\n",
       "      <th>repeat_gene</th>\n",
       "      <th>TAC_freq</th>\n",
       "      <th>GTG_freq</th>\n",
       "      <th>AGC_freq</th>\n",
       "      <th>TAG_freq</th>\n",
       "      <th>CCA_freq</th>\n",
       "      <th>GAC_freq</th>\n",
       "      <th>CTT_freq</th>\n",
       "      <th>GCA_freq</th>\n",
       "      <th>TTC_freq</th>\n",
       "      <th>TCA_freq</th>\n",
       "      <th>AGG_freq</th>\n",
       "      <th>TCC_freq</th>\n",
       "      <th>ACG_freq</th>\n",
       "      <th>AGA_freq</th>\n",
       "      <th>CGT_freq</th>\n",
       "      <th>H3k27ac_cds</th>\n",
       "      <th>GAT_freq</th>\n",
       "      <th>AAA_freq</th>\n",
       "      <th>ACC_freq</th>\n",
       "      <th>TCG_freq</th>\n",
       "      <th>GGA_freq</th>\n",
       "      <th>H3k4me1_gene</th>\n",
       "      <th>ACT_freq</th>\n",
       "      <th>ACA_freq</th>\n",
       "      <th>H3k27ac_gene</th>\n",
       "      <th>CTC_freq</th>\n",
       "      <th>CCT_freq</th>\n",
       "      <th>AAC_freq</th>\n",
       "      <th>CTA_freq</th>\n",
       "      <th>GTT_freq</th>\n",
       "      <th>CGC_freq</th>\n",
       "      <th>CAC_freq</th>\n",
       "      <th>TGA_freq</th>\n",
       "      <th>dnase_cds</th>\n",
       "      <th>GCT_freq</th>\n",
       "      <th>CAT_freq</th>\n",
       "      <th>CGG_freq</th>\n",
       "      <th>TAT_freq</th>\n",
       "      <th>CAG_freq</th>\n",
       "      <th>GAG_freq</th>\n",
       "      <th>GGC_freq</th>\n",
       "      <th>CAA_freq</th>\n",
       "      <th>TGC_freq</th>\n",
       "      <th>transcription_gene</th>\n",
       "      <th>CCC_freq</th>\n",
       "      <th>GCC_freq</th>\n",
       "      <th>GAA_freq</th>\n",
       "      <th>repeat_cds</th>\n",
       "      <th>ATA_freq</th>\n",
       "      <th>AAT_freq</th>\n",
       "      <th>CCG_freq</th>\n",
       "      <th>gc_cds</th>\n",
       "      <th>oldest_phylostratum</th>\n",
       "      <th>CTG_freq</th>\n",
       "      <th>ATT_freq</th>\n",
       "      <th>TAA_freq</th>\n",
       "      <th>ATG_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1488</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.041809</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0.809254</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612230</td>\n",
       "      <td>0.005435</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>33</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.706453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.026495</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.023777</td>\n",
       "      <td>0.561429</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.216855</td>\n",
       "      <td>0.017663</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.030571</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>6.798234</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.033967</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>0.657258</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.044837</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>873</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>-0.007148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.828752</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.086769</td>\n",
       "      <td>0.026406</td>\n",
       "      <td>0.027555</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>28</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.097018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.043350</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.035591</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.657839</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.061963</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.024110</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.010333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1092</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.040463</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>1.249600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479295</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.021536</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>36</td>\n",
       "      <td>0.754579</td>\n",
       "      <td>1.354306</td>\n",
       "      <td>0.354628</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.868383</td>\n",
       "      <td>0.028404</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.034644</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.086996</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.851369</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.618954</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.611722</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>6.081620</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.572344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.006554</td>\n",
       "      <td>0.019663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2800</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>-0.022495</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.009038</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>0.921420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>0.022415</td>\n",
       "      <td>126</td>\n",
       "      <td>0.270357</td>\n",
       "      <td>1.382249</td>\n",
       "      <td>0.052420</td>\n",
       "      <td>0.021330</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>1.143060</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>0.020607</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.023861</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.018077</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.151429</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.554023</td>\n",
       "      <td>0.015546</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.278492</td>\n",
       "      <td>0.010846</td>\n",
       "      <td>0.019161</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.031092</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.016631</td>\n",
       "      <td>0.027477</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>2.254471</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.022415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025307</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.013738</td>\n",
       "      <td>0.026392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.960747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143843</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.026099</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>55</td>\n",
       "      <td>0.708221</td>\n",
       "      <td>1.196871</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.030997</td>\n",
       "      <td>4.217000</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.659704</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.400789</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.457949</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.030997</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>1.080241</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>0.025412</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.401617</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.032280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19740</th>\n",
       "      <td>2649</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.004968</td>\n",
       "      <td>0.051321</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>0.009171</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>1.156640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313496</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>0.021781</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.020634</td>\n",
       "      <td>208</td>\n",
       "      <td>0.371461</td>\n",
       "      <td>1.677763</td>\n",
       "      <td>0.380132</td>\n",
       "      <td>0.014903</td>\n",
       "      <td>0.147603</td>\n",
       "      <td>0.549588</td>\n",
       "      <td>0.019776</td>\n",
       "      <td>0.011846</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.003057</td>\n",
       "      <td>0.026748</td>\n",
       "      <td>0.019870</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.010699</td>\n",
       "      <td>0.015667</td>\n",
       "      <td>0.019488</td>\n",
       "      <td>0.019488</td>\n",
       "      <td>0.017195</td>\n",
       "      <td>0.012228</td>\n",
       "      <td>0.022163</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>0.375613</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>0.018342</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.022927</td>\n",
       "      <td>0.721323</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>0.029041</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.017577</td>\n",
       "      <td>0.021016</td>\n",
       "      <td>0.015285</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>0.025984</td>\n",
       "      <td>0.028659</td>\n",
       "      <td>0.427709</td>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>0.026366</td>\n",
       "      <td>0.022545</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.020634</td>\n",
       "      <td>0.015667</td>\n",
       "      <td>12.624956</td>\n",
       "      <td>0.023691</td>\n",
       "      <td>0.021781</td>\n",
       "      <td>0.024838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>0.015667</td>\n",
       "      <td>0.009935</td>\n",
       "      <td>0.514911</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030187</td>\n",
       "      <td>0.015667</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>0.015667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19741</th>\n",
       "      <td>4035</td>\n",
       "      <td>0.013830</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>0.032907</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.009303</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.011315</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>0.952004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159518</td>\n",
       "      <td>0.018858</td>\n",
       "      <td>0.027910</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>0.021624</td>\n",
       "      <td>88</td>\n",
       "      <td>0.538290</td>\n",
       "      <td>1.596068</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016595</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.271970</td>\n",
       "      <td>0.013269</td>\n",
       "      <td>0.011567</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.024642</td>\n",
       "      <td>0.014332</td>\n",
       "      <td>0.022630</td>\n",
       "      <td>0.016595</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.019864</td>\n",
       "      <td>0.015841</td>\n",
       "      <td>0.018104</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>0.018607</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.578686</td>\n",
       "      <td>0.016595</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.018607</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>0.019613</td>\n",
       "      <td>0.618466</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>0.021624</td>\n",
       "      <td>0.379258</td>\n",
       "      <td>0.015590</td>\n",
       "      <td>0.018104</td>\n",
       "      <td>0.013327</td>\n",
       "      <td>0.012069</td>\n",
       "      <td>0.010561</td>\n",
       "      <td>0.004275</td>\n",
       "      <td>0.019864</td>\n",
       "      <td>0.022630</td>\n",
       "      <td>0.305328</td>\n",
       "      <td>0.018356</td>\n",
       "      <td>0.023133</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>0.012321</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.015841</td>\n",
       "      <td>0.019361</td>\n",
       "      <td>0.017098</td>\n",
       "      <td>4.338614</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.015590</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.012824</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>0.486245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021121</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.008801</td>\n",
       "      <td>0.025396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19742</th>\n",
       "      <td>2043</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.045040</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.033450</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.007489</td>\n",
       "      <td>0.018472</td>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.865913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164623</td>\n",
       "      <td>0.011483</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.009486</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>37</td>\n",
       "      <td>0.785120</td>\n",
       "      <td>1.245576</td>\n",
       "      <td>0.710461</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.786099</td>\n",
       "      <td>2.458350</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>0.014478</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.031952</td>\n",
       "      <td>0.013979</td>\n",
       "      <td>0.034448</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.008987</td>\n",
       "      <td>0.019970</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>0.016975</td>\n",
       "      <td>0.006490</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.748995</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>0.015477</td>\n",
       "      <td>0.872609</td>\n",
       "      <td>0.020969</td>\n",
       "      <td>0.025462</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>0.014978</td>\n",
       "      <td>0.012981</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.015976</td>\n",
       "      <td>0.018972</td>\n",
       "      <td>0.025453</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.018472</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>0.016475</td>\n",
       "      <td>0.024963</td>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.011982</td>\n",
       "      <td>0.017474</td>\n",
       "      <td>0.015976</td>\n",
       "      <td>7.591840</td>\n",
       "      <td>0.012481</td>\n",
       "      <td>0.013979</td>\n",
       "      <td>0.021468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012481</td>\n",
       "      <td>0.024463</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>0.443465</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.024963</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.013979</td>\n",
       "      <td>0.024463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19743</th>\n",
       "      <td>372</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.017871</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>1.277585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166620</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.767925</td>\n",
       "      <td>0.861899</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.001840</td>\n",
       "      <td>0.020090</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.035135</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.032432</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.857123</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.572581</td>\n",
       "      <td>0.010811</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>0.021622</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.005405</td>\n",
       "      <td>0.136402</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.016216</td>\n",
       "      <td>0.035135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.462366</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.024324</td>\n",
       "      <td>0.018919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19744</th>\n",
       "      <td>1994</td>\n",
       "      <td>0.009645</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.047280</td>\n",
       "      <td>0.019797</td>\n",
       "      <td>0.016751</td>\n",
       "      <td>0.014721</td>\n",
       "      <td>0.017766</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.011168</td>\n",
       "      <td>2.501995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362039</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>0.025888</td>\n",
       "      <td>0.023350</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>53</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.685192</td>\n",
       "      <td>0.789180</td>\n",
       "      <td>0.007614</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>2.421960</td>\n",
       "      <td>0.027329</td>\n",
       "      <td>0.009645</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.025888</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.031980</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.014721</td>\n",
       "      <td>0.023350</td>\n",
       "      <td>0.017259</td>\n",
       "      <td>0.012690</td>\n",
       "      <td>0.018274</td>\n",
       "      <td>0.019797</td>\n",
       "      <td>0.015736</td>\n",
       "      <td>0.013198</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.511535</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>0.023858</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>0.024365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012183</td>\n",
       "      <td>0.009137</td>\n",
       "      <td>0.721653</td>\n",
       "      <td>0.019797</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>0.008122</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>0.019289</td>\n",
       "      <td>0.021320</td>\n",
       "      <td>0.013198</td>\n",
       "      <td>0.633400</td>\n",
       "      <td>0.025381</td>\n",
       "      <td>0.013706</td>\n",
       "      <td>0.018782</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.031472</td>\n",
       "      <td>0.024365</td>\n",
       "      <td>0.026396</td>\n",
       "      <td>0.010152</td>\n",
       "      <td>0.029442</td>\n",
       "      <td>11.936696</td>\n",
       "      <td>0.029949</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.013198</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.015736</td>\n",
       "      <td>0.620863</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.039594</td>\n",
       "      <td>0.004569</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.013198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18170 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       cds_length  AGT_freq  GTA_freq  repli_gene  GCG_freq  TCT_freq  \\\n",
       "0            1488  0.009511  0.000679    0.041809  0.027853  0.010190   \n",
       "1             873  0.006889  0.005741   -0.007148  0.000000  0.022962   \n",
       "2            1092  0.011236  0.008427    0.040463  0.007491  0.008427   \n",
       "3            2800  0.015907  0.009400   -0.022495  0.004700  0.009038   \n",
       "4            1484  0.009615  0.009615   -0.000387  0.002747  0.015797   \n",
       "...           ...       ...       ...         ...       ...       ...   \n",
       "19740        2649  0.009171  0.004968    0.051321  0.005732  0.017577   \n",
       "19741        4035  0.013830  0.008046    0.032907  0.004275  0.018858   \n",
       "19742        2043  0.014978  0.004993    0.045040  0.001498  0.033450   \n",
       "19743         372  0.008108  0.010811    0.017871  0.005405  0.018919   \n",
       "19744        1994  0.009645  0.003046    0.047280  0.019797  0.016751   \n",
       "\n",
       "       CGA_freq  TGT_freq  GGT_freq  TTA_freq  GTC_freq  nsome_gene  \\\n",
       "0      0.015625  0.010870  0.013587  0.000679  0.012908    0.809254   \n",
       "1      0.003444  0.011481  0.018370  0.021814  0.008037    0.828752   \n",
       "2      0.005618  0.010300  0.013109  0.004682  0.010300    1.249600   \n",
       "3      0.005785  0.016992  0.011931  0.016269  0.007231    0.921420   \n",
       "4      0.005495  0.018544  0.008242  0.014423  0.004808    0.960747   \n",
       "...         ...       ...       ...       ...       ...         ...   \n",
       "19740  0.009171  0.011464  0.012992  0.006496  0.011846    1.156640   \n",
       "19741  0.009303  0.016093  0.014835  0.011315  0.011064    0.952004   \n",
       "19742  0.004493  0.014978  0.007489  0.018472  0.010484    0.865913   \n",
       "19743  0.005405  0.018919  0.010811  0.010811  0.010811    1.277585   \n",
       "19744  0.014721  0.017766  0.012690  0.001523  0.011168    2.501995   \n",
       "\n",
       "       lamin_gene  dnase_gene  TTG_freq  TGG_freq  GGG_freq  AAG_freq  \\\n",
       "0             0.0    0.612230  0.005435  0.029212  0.029212  0.007473   \n",
       "1             1.0    0.086769  0.026406  0.027555  0.017222  0.021814   \n",
       "2             0.0    0.479295  0.004682  0.029026  0.021536  0.023408   \n",
       "3             0.0    0.171524  0.015907  0.019523  0.010123  0.022415   \n",
       "4             0.0    0.143843  0.019231  0.026099  0.010302  0.030220   \n",
       "...           ...         ...       ...       ...       ...       ...   \n",
       "19740         0.0    0.313496  0.014138  0.021781  0.011846  0.020634   \n",
       "19741         0.0    0.159518  0.018858  0.027910  0.016093  0.021624   \n",
       "19742         0.0    0.164623  0.011483  0.021468  0.009486  0.020469   \n",
       "19743         0.0    0.166620  0.016216  0.027027  0.005405  0.027027   \n",
       "19744         0.0    0.362039  0.009137  0.025888  0.023350  0.012690   \n",
       "\n",
       "       occ_total_sum  H3k4me1_cds  nsome_cds  H3k4me3_gene  ATC_freq  \\\n",
       "0                 33     0.661290   0.706453      1.000000  0.006114   \n",
       "1                 28     0.000000   1.097018      0.000000  0.017222   \n",
       "2                 36     0.754579   1.354306      0.354628  0.013109   \n",
       "3                126     0.270357   1.382249      0.052420  0.021330   \n",
       "4                 55     0.708221   1.196871      0.106455  0.009615   \n",
       "...              ...          ...        ...           ...       ...   \n",
       "19740            208     0.371461   1.677763      0.380132  0.014903   \n",
       "19741             88     0.538290   1.596068      1.000000  0.016595   \n",
       "19742             37     0.785120   1.245576      0.710461  0.019970   \n",
       "19743             14     1.000000   1.767925      0.861899  0.027027   \n",
       "19744             53     1.000000   2.685192      0.789180  0.007614   \n",
       "\n",
       "       H3k4me3_cds  recomb_gene  repeat_gene  TAC_freq  GTG_freq  AGC_freq  \\\n",
       "0         1.000000     0.000000     0.040516  0.008152  0.027174  0.028533   \n",
       "1         0.000000     2.043350     0.002809  0.014925  0.012629  0.009185   \n",
       "2         0.030220     0.868383     0.028404  0.012172  0.016854  0.020599   \n",
       "3         0.021429     1.143060     0.014520  0.008315  0.020607  0.016992   \n",
       "4         0.030997     4.217000     0.009545  0.012363  0.013736  0.006868   \n",
       "...            ...          ...          ...       ...       ...       ...   \n",
       "19740     0.147603     0.549588     0.019776  0.011846  0.017577  0.018724   \n",
       "19741     1.000000     2.271970     0.013269  0.011567  0.019361  0.016847   \n",
       "19742     0.786099     2.458350     0.014049  0.014478  0.012981  0.017474   \n",
       "19743     1.000000     2.001840     0.020090  0.018919  0.013514  0.013514   \n",
       "19744     0.266800     2.421960     0.027329  0.009645  0.023858  0.025888   \n",
       "\n",
       "       TAG_freq  CCA_freq  GAC_freq  CTT_freq  GCA_freq  TTC_freq  TCA_freq  \\\n",
       "0      0.000000  0.027174  0.021739  0.013587  0.016984  0.013587  0.008832   \n",
       "1      0.006889  0.017222  0.013777  0.019518  0.014925  0.017222  0.017222   \n",
       "2      0.005618  0.034644  0.022472  0.011236  0.015918  0.010300  0.014981   \n",
       "3      0.006146  0.023861  0.022777  0.015907  0.015184  0.014100  0.022054   \n",
       "4      0.008242  0.015110  0.017857  0.013736  0.016484  0.015797  0.015110   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19740  0.003057  0.026748  0.019870  0.012610  0.010699  0.015667  0.019488   \n",
       "19741  0.006035  0.024642  0.014332  0.022630  0.016595  0.019613  0.019864   \n",
       "19742  0.005991  0.021468  0.008987  0.031952  0.013979  0.034448  0.025961   \n",
       "19743  0.005405  0.035135  0.016216  0.016216  0.008108  0.016216  0.016216   \n",
       "19744  0.001015  0.031980  0.021320  0.014721  0.023350  0.017259  0.012690   \n",
       "\n",
       "       AGG_freq  TCC_freq  ACG_freq  AGA_freq  CGT_freq  H3k27ac_cds  \\\n",
       "0      0.019701  0.021739  0.010870  0.010190  0.009511     0.198925   \n",
       "1      0.016073  0.016073  0.003444  0.035591  0.002296     0.000000   \n",
       "2      0.025281  0.016854  0.007491  0.024345  0.003745     0.086996   \n",
       "3      0.012292  0.018077  0.006146  0.024946  0.003977     0.151429   \n",
       "4      0.015797  0.012363  0.002747  0.048077  0.002060     0.659704   \n",
       "...         ...       ...       ...       ...       ...          ...   \n",
       "19740  0.019488  0.017195  0.012228  0.022163  0.007642     0.375613   \n",
       "19741  0.015841  0.018104  0.004275  0.018607  0.003017     0.578686   \n",
       "19742  0.008987  0.019970  0.002996  0.017474  0.003495     1.000000   \n",
       "19743  0.013514  0.024324  0.008108  0.032432  0.005405     1.000000   \n",
       "19744  0.018274  0.019797  0.015736  0.013198  0.005076     0.511535   \n",
       "\n",
       "       GAT_freq  AAA_freq  ACC_freq  TCG_freq  GGA_freq  H3k4me1_gene  \\\n",
       "0      0.008152  0.004755  0.026495  0.009511  0.023777      0.561429   \n",
       "1      0.011481  0.025258  0.018370  0.005741  0.018370      0.657839   \n",
       "2      0.009363  0.018727  0.024345  0.009363  0.028090      0.851369   \n",
       "3      0.018800  0.022054  0.014100  0.004700  0.017354      0.554023   \n",
       "4      0.024725  0.039835  0.012363  0.002060  0.022665      0.400789   \n",
       "...         ...       ...       ...       ...       ...           ...   \n",
       "19740  0.016431  0.016813  0.018342  0.005732  0.022927      0.721323   \n",
       "19741  0.016595  0.019613  0.018607  0.006035  0.019613      0.618466   \n",
       "19742  0.014978  0.024463  0.016975  0.006490  0.014978      0.748995   \n",
       "19743  0.010811  0.018919  0.013514  0.005405  0.018919      0.857123   \n",
       "19744  0.006599  0.004061  0.023858  0.006599  0.024365      1.000000   \n",
       "\n",
       "       ACT_freq  ACA_freq  H3k27ac_gene  CTC_freq  CCT_freq  AAC_freq  \\\n",
       "0      0.008152  0.011549      0.216855  0.017663  0.029891  0.008152   \n",
       "1      0.012629  0.025258      0.000000  0.019518  0.021814  0.019518   \n",
       "2      0.014981  0.017790      0.618954  0.015918  0.028090  0.012172   \n",
       "3      0.015546  0.022054      0.278492  0.010846  0.019161  0.014823   \n",
       "4      0.019918  0.024038      0.457949  0.014423  0.013049  0.015797   \n",
       "...         ...       ...           ...       ...       ...       ...   \n",
       "19740  0.012992  0.029041      0.560000  0.017577  0.021016  0.015285   \n",
       "19741  0.016093  0.021624      0.379258  0.015590  0.018104  0.013327   \n",
       "19742  0.011982  0.015477      0.872609  0.020969  0.025462  0.007988   \n",
       "19743  0.018919  0.027027      1.000000  0.010811  0.016216  0.013514   \n",
       "19744  0.012183  0.009137      0.721653  0.019797  0.028934  0.008122   \n",
       "\n",
       "       CTA_freq  GTT_freq  CGC_freq  CAC_freq  TGA_freq  dnase_cds  GCT_freq  \\\n",
       "0      0.007473  0.003397  0.027174  0.019022  0.020380   0.758065  0.034647   \n",
       "1      0.008037  0.012629  0.001148  0.011481  0.020666   0.195876  0.012629   \n",
       "2      0.015918  0.003745  0.010300  0.016854  0.014981   0.611722  0.025281   \n",
       "3      0.006146  0.011931  0.003977  0.011931  0.031092   0.280357  0.014461   \n",
       "4      0.013736  0.011676  0.001374  0.012363  0.024038   0.030997  0.006181   \n",
       "...         ...       ...       ...       ...       ...        ...       ...   \n",
       "19740  0.009935  0.007642  0.007642  0.025984  0.028659   0.427709  0.018724   \n",
       "19741  0.012069  0.010561  0.004275  0.019864  0.022630   0.305328  0.018356   \n",
       "19742  0.014978  0.012981  0.004493  0.015976  0.018972   0.025453  0.021468   \n",
       "19743  0.013514  0.008108  0.005405  0.018919  0.029730   0.572581  0.010811   \n",
       "19744  0.008629  0.008629  0.019289  0.021320  0.013198   0.633400  0.025381   \n",
       "\n",
       "       CAT_freq  CGG_freq  TAT_freq  CAG_freq  GAG_freq  GGC_freq  CAA_freq  \\\n",
       "0      0.007473  0.019701  0.001359  0.028533  0.031250  0.030571  0.009511   \n",
       "1      0.022962  0.004592  0.017222  0.020666  0.019518  0.011481  0.019518   \n",
       "2      0.017790  0.014045  0.008427  0.029963  0.026217  0.029026  0.017790   \n",
       "3      0.017354  0.007954  0.016631  0.027477  0.016269  0.010484  0.026392   \n",
       "4      0.015797  0.002060  0.019231  0.021978  0.024725  0.013049  0.019918   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19740  0.012992  0.008789  0.007642  0.026366  0.022545  0.014520  0.020634   \n",
       "19741  0.023133  0.007040  0.012321  0.020619  0.019361  0.015841  0.019361   \n",
       "19742  0.018472  0.003994  0.016475  0.024963  0.010484  0.011982  0.017474   \n",
       "19743  0.024324  0.005405  0.013514  0.013514  0.021622  0.016216  0.029730   \n",
       "19744  0.013706  0.018782  0.002030  0.031472  0.024365  0.026396  0.010152   \n",
       "\n",
       "       TGC_freq  transcription_gene  CCC_freq  GCC_freq  GAA_freq  repeat_cds  \\\n",
       "0      0.027174            6.798234  0.031250  0.033967  0.008832         0.0   \n",
       "1      0.012629            0.061963  0.008037  0.006889  0.033295         0.0   \n",
       "2      0.019663            6.081620  0.022472  0.031835  0.014981         0.0   \n",
       "3      0.019523            2.254471  0.016992  0.016992  0.022415         0.0   \n",
       "4      0.010989            1.080241  0.003434  0.006868  0.034341         0.0   \n",
       "...         ...                 ...       ...       ...       ...         ...   \n",
       "19740  0.015667           12.624956  0.023691  0.021781  0.024838         0.0   \n",
       "19741  0.017098            4.338614  0.014835  0.015590  0.020116         0.0   \n",
       "19742  0.015976            7.591840  0.012481  0.013979  0.021468         0.0   \n",
       "19743  0.005405            0.136402  0.016216  0.016216  0.035135         0.0   \n",
       "19744  0.029442           11.936696  0.029949  0.032995  0.013198         0.0   \n",
       "\n",
       "       ATA_freq  AAT_freq  CCG_freq    gc_cds  oldest_phylostratum  CTG_freq  \\\n",
       "0      0.000679  0.002717  0.025136  0.657258                 12.0  0.044837   \n",
       "1      0.016073  0.024110  0.002296  0.422680                  1.0  0.022962   \n",
       "2      0.003745  0.003745  0.010300  0.572344                  1.0  0.033708   \n",
       "3      0.013377  0.024946  0.006508  0.460000                  1.0  0.025307   \n",
       "4      0.020604  0.025412  0.004121  0.401617                  1.0  0.014423   \n",
       "...         ...       ...       ...       ...                  ...       ...   \n",
       "19740  0.007642  0.015667  0.009935  0.514911                  1.0  0.030187   \n",
       "19741  0.007292  0.012824  0.009052  0.486245                  1.0  0.021121   \n",
       "19742  0.012481  0.024463  0.003994  0.443465                  2.0  0.024963   \n",
       "19743  0.002703  0.024324  0.002703  0.462366                 12.0  0.024324   \n",
       "19744  0.001015  0.002538  0.015736  0.620863                  6.0  0.039594   \n",
       "\n",
       "       ATT_freq  TAA_freq  ATG_freq  \n",
       "0      0.002038  0.000000  0.010870  \n",
       "1      0.033295  0.012629  0.010333  \n",
       "2      0.004682  0.006554  0.019663  \n",
       "3      0.017715  0.013738  0.026392  \n",
       "4      0.023352  0.018544  0.032280  \n",
       "...         ...       ...       ...  \n",
       "19740  0.015667  0.006878  0.015667  \n",
       "19741  0.016847  0.008801  0.025396  \n",
       "19742  0.019471  0.013979  0.024463  \n",
       "19743  0.018919  0.000000  0.027027  \n",
       "19744  0.004569  0.001523  0.013198  \n",
       "\n",
       "[18170 rows x 83 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns='response', inplace=True)\n",
    "df.iloc[:, importance_df.sort_values(by=\"Score\", key=abs, ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffeca2de-0fd7-48d7-8d65-6e485eeb2ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-1.029191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.896040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.855108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.771229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-0.750641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.731362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-0.698490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.696801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.684703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.684126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score\n",
       "79 -1.029191\n",
       "66  0.896040\n",
       "3   0.855108\n",
       "45 -0.771229\n",
       "77 -0.750641\n",
       "71  0.731362\n",
       "46 -0.698490\n",
       "50  0.696801\n",
       "31 -0.684703\n",
       "58  0.684126"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df = pd.DataFrame(np.reshape(lr_clf[\"lr\"].coef_, (lr_clf[\"lr\"].coef_.shape[1], 1)), columns=[\"Score\"])\n",
    "top_features_lr = importance_df.sort_values(by=\"Score\", key=abs, ascending=False).index.to_numpy()\n",
    "importance_df.sort_values(by=\"Score\", key=abs, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f9e876cb-47c0-4282-adf9-ae933bb6b736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>cds_length</th>\n",
       "      <th>occ_total_sum</th>\n",
       "      <th>oldest_phylostratum</th>\n",
       "      <th>gc_cds</th>\n",
       "      <th>dnase_gene</th>\n",
       "      <th>dnase_cds</th>\n",
       "      <th>H3k4me1_gene</th>\n",
       "      <th>H3k4me3_gene</th>\n",
       "      <th>H3k27ac_gene</th>\n",
       "      <th>H3k4me1_cds</th>\n",
       "      <th>H3k4me3_cds</th>\n",
       "      <th>H3k27ac_cds</th>\n",
       "      <th>lamin_gene</th>\n",
       "      <th>repli_gene</th>\n",
       "      <th>nsome_gene</th>\n",
       "      <th>nsome_cds</th>\n",
       "      <th>transcription_gene</th>\n",
       "      <th>repeat_gene</th>\n",
       "      <th>repeat_cds</th>\n",
       "      <th>recomb_gene</th>\n",
       "      <th>AAA_freq</th>\n",
       "      <th>AAC_freq</th>\n",
       "      <th>AAG_freq</th>\n",
       "      <th>AAT_freq</th>\n",
       "      <th>ACA_freq</th>\n",
       "      <th>ACC_freq</th>\n",
       "      <th>ACG_freq</th>\n",
       "      <th>ACT_freq</th>\n",
       "      <th>AGA_freq</th>\n",
       "      <th>AGC_freq</th>\n",
       "      <th>AGG_freq</th>\n",
       "      <th>AGT_freq</th>\n",
       "      <th>ATA_freq</th>\n",
       "      <th>ATC_freq</th>\n",
       "      <th>ATG_freq</th>\n",
       "      <th>ATT_freq</th>\n",
       "      <th>CAA_freq</th>\n",
       "      <th>CAC_freq</th>\n",
       "      <th>CAG_freq</th>\n",
       "      <th>CAT_freq</th>\n",
       "      <th>CCA_freq</th>\n",
       "      <th>CCC_freq</th>\n",
       "      <th>CCG_freq</th>\n",
       "      <th>CCT_freq</th>\n",
       "      <th>CGA_freq</th>\n",
       "      <th>CGC_freq</th>\n",
       "      <th>CGG_freq</th>\n",
       "      <th>CGT_freq</th>\n",
       "      <th>CTA_freq</th>\n",
       "      <th>CTC_freq</th>\n",
       "      <th>CTG_freq</th>\n",
       "      <th>CTT_freq</th>\n",
       "      <th>GAA_freq</th>\n",
       "      <th>GAC_freq</th>\n",
       "      <th>GAG_freq</th>\n",
       "      <th>GAT_freq</th>\n",
       "      <th>GCA_freq</th>\n",
       "      <th>GCC_freq</th>\n",
       "      <th>GCG_freq</th>\n",
       "      <th>GCT_freq</th>\n",
       "      <th>GGA_freq</th>\n",
       "      <th>GGC_freq</th>\n",
       "      <th>GGG_freq</th>\n",
       "      <th>GGT_freq</th>\n",
       "      <th>GTA_freq</th>\n",
       "      <th>GTC_freq</th>\n",
       "      <th>GTG_freq</th>\n",
       "      <th>GTT_freq</th>\n",
       "      <th>TAA_freq</th>\n",
       "      <th>TAC_freq</th>\n",
       "      <th>TAG_freq</th>\n",
       "      <th>TAT_freq</th>\n",
       "      <th>TCA_freq</th>\n",
       "      <th>TCC_freq</th>\n",
       "      <th>TCG_freq</th>\n",
       "      <th>TCT_freq</th>\n",
       "      <th>TGA_freq</th>\n",
       "      <th>TGC_freq</th>\n",
       "      <th>TGG_freq</th>\n",
       "      <th>TGT_freq</th>\n",
       "      <th>TTA_freq</th>\n",
       "      <th>TTC_freq</th>\n",
       "      <th>TTG_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1488</td>\n",
       "      <td>33</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.657258</td>\n",
       "      <td>0.612230</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.561429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216855</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041809</td>\n",
       "      <td>0.809254</td>\n",
       "      <td>0.706453</td>\n",
       "      <td>6.798234</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.002717</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>0.026495</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.019022</td>\n",
       "      <td>0.028533</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.019701</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.007473</td>\n",
       "      <td>0.017663</td>\n",
       "      <td>0.044837</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.016984</td>\n",
       "      <td>0.033967</td>\n",
       "      <td>0.027853</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>0.023777</td>\n",
       "      <td>0.030571</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.003397</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.010190</td>\n",
       "      <td>0.020380</td>\n",
       "      <td>0.027174</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.013587</td>\n",
       "      <td>0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>873</td>\n",
       "      <td>28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.086769</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.657839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.007148</td>\n",
       "      <td>0.828752</td>\n",
       "      <td>1.097018</td>\n",
       "      <td>0.061963</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.043350</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.024110</td>\n",
       "      <td>0.025258</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.035591</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.010333</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.033295</td>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.016073</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.012629</td>\n",
       "      <td>0.027555</td>\n",
       "      <td>0.011481</td>\n",
       "      <td>0.021814</td>\n",
       "      <td>0.017222</td>\n",
       "      <td>0.026406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1092</td>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572344</td>\n",
       "      <td>0.479295</td>\n",
       "      <td>0.611722</td>\n",
       "      <td>0.851369</td>\n",
       "      <td>0.354628</td>\n",
       "      <td>0.618954</td>\n",
       "      <td>0.754579</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.086996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040463</td>\n",
       "      <td>1.249600</td>\n",
       "      <td>1.354306</td>\n",
       "      <td>6.081620</td>\n",
       "      <td>0.028404</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.868383</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.024345</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.029963</td>\n",
       "      <td>0.017790</td>\n",
       "      <td>0.034644</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.033708</td>\n",
       "      <td>0.011236</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.015918</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.028090</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.021536</td>\n",
       "      <td>0.013109</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.006554</td>\n",
       "      <td>0.012172</td>\n",
       "      <td>0.005618</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.016854</td>\n",
       "      <td>0.009363</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>0.014981</td>\n",
       "      <td>0.019663</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.004682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2800</td>\n",
       "      <td>126</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.171524</td>\n",
       "      <td>0.280357</td>\n",
       "      <td>0.554023</td>\n",
       "      <td>0.052420</td>\n",
       "      <td>0.278492</td>\n",
       "      <td>0.270357</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.151429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.022495</td>\n",
       "      <td>0.921420</td>\n",
       "      <td>1.382249</td>\n",
       "      <td>2.254471</td>\n",
       "      <td>0.014520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.143060</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.014823</td>\n",
       "      <td>0.022415</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.015546</td>\n",
       "      <td>0.024946</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.013377</td>\n",
       "      <td>0.021330</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.027477</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.023861</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>0.019161</td>\n",
       "      <td>0.005785</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.010846</td>\n",
       "      <td>0.025307</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.022415</td>\n",
       "      <td>0.022777</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.010484</td>\n",
       "      <td>0.010123</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.007231</td>\n",
       "      <td>0.020607</td>\n",
       "      <td>0.011931</td>\n",
       "      <td>0.013738</td>\n",
       "      <td>0.008315</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.016631</td>\n",
       "      <td>0.022054</td>\n",
       "      <td>0.018077</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.009038</td>\n",
       "      <td>0.031092</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.019523</td>\n",
       "      <td>0.016992</td>\n",
       "      <td>0.016269</td>\n",
       "      <td>0.014100</td>\n",
       "      <td>0.015907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1484</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.401617</td>\n",
       "      <td>0.143843</td>\n",
       "      <td>0.030997</td>\n",
       "      <td>0.400789</td>\n",
       "      <td>0.106455</td>\n",
       "      <td>0.457949</td>\n",
       "      <td>0.708221</td>\n",
       "      <td>0.030997</td>\n",
       "      <td>0.659704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>0.960747</td>\n",
       "      <td>1.196871</td>\n",
       "      <td>1.080241</td>\n",
       "      <td>0.009545</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.217000</td>\n",
       "      <td>0.039835</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.030220</td>\n",
       "      <td>0.025412</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.048077</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.032280</td>\n",
       "      <td>0.023352</td>\n",
       "      <td>0.019918</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.003434</td>\n",
       "      <td>0.004121</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.034341</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.024725</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.013049</td>\n",
       "      <td>0.010302</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.013736</td>\n",
       "      <td>0.011676</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.015110</td>\n",
       "      <td>0.012363</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.024038</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.026099</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.014423</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.019231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response  cds_length  occ_total_sum  oldest_phylostratum    gc_cds  \\\n",
       "0         0        1488             33                 12.0  0.657258   \n",
       "1         0         873             28                  1.0  0.422680   \n",
       "2         1        1092             36                  1.0  0.572344   \n",
       "3         0        2800            126                  1.0  0.460000   \n",
       "4         1        1484             55                  1.0  0.401617   \n",
       "\n",
       "   dnase_gene  dnase_cds  H3k4me1_gene  H3k4me3_gene  H3k27ac_gene  \\\n",
       "0    0.612230   0.758065      0.561429      1.000000      0.216855   \n",
       "1    0.086769   0.195876      0.657839      0.000000      0.000000   \n",
       "2    0.479295   0.611722      0.851369      0.354628      0.618954   \n",
       "3    0.171524   0.280357      0.554023      0.052420      0.278492   \n",
       "4    0.143843   0.030997      0.400789      0.106455      0.457949   \n",
       "\n",
       "   H3k4me1_cds  H3k4me3_cds  H3k27ac_cds  lamin_gene  repli_gene  nsome_gene  \\\n",
       "0     0.661290     1.000000     0.198925         0.0    0.041809    0.809254   \n",
       "1     0.000000     0.000000     0.000000         1.0   -0.007148    0.828752   \n",
       "2     0.754579     0.030220     0.086996         0.0    0.040463    1.249600   \n",
       "3     0.270357     0.021429     0.151429         0.0   -0.022495    0.921420   \n",
       "4     0.708221     0.030997     0.659704         0.0   -0.000387    0.960747   \n",
       "\n",
       "   nsome_cds  transcription_gene  repeat_gene  repeat_cds  recomb_gene  \\\n",
       "0   0.706453            6.798234     0.040516         0.0     0.000000   \n",
       "1   1.097018            0.061963     0.002809         0.0     2.043350   \n",
       "2   1.354306            6.081620     0.028404         0.0     0.868383   \n",
       "3   1.382249            2.254471     0.014520         0.0     1.143060   \n",
       "4   1.196871            1.080241     0.009545         0.0     4.217000   \n",
       "\n",
       "   AAA_freq  AAC_freq  AAG_freq  AAT_freq  ACA_freq  ACC_freq  ACG_freq  \\\n",
       "0  0.004755  0.008152  0.007473  0.002717  0.011549  0.026495  0.010870   \n",
       "1  0.025258  0.019518  0.021814  0.024110  0.025258  0.018370  0.003444   \n",
       "2  0.018727  0.012172  0.023408  0.003745  0.017790  0.024345  0.007491   \n",
       "3  0.022054  0.014823  0.022415  0.024946  0.022054  0.014100  0.006146   \n",
       "4  0.039835  0.015797  0.030220  0.025412  0.024038  0.012363  0.002747   \n",
       "\n",
       "   ACT_freq  AGA_freq  AGC_freq  AGG_freq  AGT_freq  ATA_freq  ATC_freq  \\\n",
       "0  0.008152  0.010190  0.028533  0.019701  0.009511  0.000679  0.006114   \n",
       "1  0.012629  0.035591  0.009185  0.016073  0.006889  0.016073  0.017222   \n",
       "2  0.014981  0.024345  0.020599  0.025281  0.011236  0.003745  0.013109   \n",
       "3  0.015546  0.024946  0.016992  0.012292  0.015907  0.013377  0.021330   \n",
       "4  0.019918  0.048077  0.006868  0.015797  0.009615  0.020604  0.009615   \n",
       "\n",
       "   ATG_freq  ATT_freq  CAA_freq  CAC_freq  CAG_freq  CAT_freq  CCA_freq  \\\n",
       "0  0.010870  0.002038  0.009511  0.019022  0.028533  0.007473  0.027174   \n",
       "1  0.010333  0.033295  0.019518  0.011481  0.020666  0.022962  0.017222   \n",
       "2  0.019663  0.004682  0.017790  0.016854  0.029963  0.017790  0.034644   \n",
       "3  0.026392  0.017715  0.026392  0.011931  0.027477  0.017354  0.023861   \n",
       "4  0.032280  0.023352  0.019918  0.012363  0.021978  0.015797  0.015110   \n",
       "\n",
       "   CCC_freq  CCG_freq  CCT_freq  CGA_freq  CGC_freq  CGG_freq  CGT_freq  \\\n",
       "0  0.031250  0.025136  0.029891  0.015625  0.027174  0.019701  0.009511   \n",
       "1  0.008037  0.002296  0.021814  0.003444  0.001148  0.004592  0.002296   \n",
       "2  0.022472  0.010300  0.028090  0.005618  0.010300  0.014045  0.003745   \n",
       "3  0.016992  0.006508  0.019161  0.005785  0.003977  0.007954  0.003977   \n",
       "4  0.003434  0.004121  0.013049  0.005495  0.001374  0.002060  0.002060   \n",
       "\n",
       "   CTA_freq  CTC_freq  CTG_freq  CTT_freq  GAA_freq  GAC_freq  GAG_freq  \\\n",
       "0  0.007473  0.017663  0.044837  0.013587  0.008832  0.021739  0.031250   \n",
       "1  0.008037  0.019518  0.022962  0.019518  0.033295  0.013777  0.019518   \n",
       "2  0.015918  0.015918  0.033708  0.011236  0.014981  0.022472  0.026217   \n",
       "3  0.006146  0.010846  0.025307  0.015907  0.022415  0.022777  0.016269   \n",
       "4  0.013736  0.014423  0.014423  0.013736  0.034341  0.017857  0.024725   \n",
       "\n",
       "   GAT_freq  GCA_freq  GCC_freq  GCG_freq  GCT_freq  GGA_freq  GGC_freq  \\\n",
       "0  0.008152  0.016984  0.033967  0.027853  0.034647  0.023777  0.030571   \n",
       "1  0.011481  0.014925  0.006889  0.000000  0.012629  0.018370  0.011481   \n",
       "2  0.009363  0.015918  0.031835  0.007491  0.025281  0.028090  0.029026   \n",
       "3  0.018800  0.015184  0.016992  0.004700  0.014461  0.017354  0.010484   \n",
       "4  0.024725  0.016484  0.006868  0.002747  0.006181  0.022665  0.013049   \n",
       "\n",
       "   GGG_freq  GGT_freq  GTA_freq  GTC_freq  GTG_freq  GTT_freq  TAA_freq  \\\n",
       "0  0.029212  0.013587  0.000679  0.012908  0.027174  0.003397  0.000000   \n",
       "1  0.017222  0.018370  0.005741  0.008037  0.012629  0.012629  0.012629   \n",
       "2  0.021536  0.013109  0.008427  0.010300  0.016854  0.003745  0.006554   \n",
       "3  0.010123  0.011931  0.009400  0.007231  0.020607  0.011931  0.013738   \n",
       "4  0.010302  0.008242  0.009615  0.004808  0.013736  0.011676  0.018544   \n",
       "\n",
       "   TAC_freq  TAG_freq  TAT_freq  TCA_freq  TCC_freq  TCG_freq  TCT_freq  \\\n",
       "0  0.008152  0.000000  0.001359  0.008832  0.021739  0.009511  0.010190   \n",
       "1  0.014925  0.006889  0.017222  0.017222  0.016073  0.005741  0.022962   \n",
       "2  0.012172  0.005618  0.008427  0.014981  0.016854  0.009363  0.008427   \n",
       "3  0.008315  0.006146  0.016631  0.022054  0.018077  0.004700  0.009038   \n",
       "4  0.012363  0.008242  0.019231  0.015110  0.012363  0.002060  0.015797   \n",
       "\n",
       "   TGA_freq  TGC_freq  TGG_freq  TGT_freq  TTA_freq  TTC_freq  TTG_freq  \n",
       "0  0.020380  0.027174  0.029212  0.010870  0.000679  0.013587  0.005435  \n",
       "1  0.020666  0.012629  0.027555  0.011481  0.021814  0.017222  0.026406  \n",
       "2  0.014981  0.019663  0.029026  0.010300  0.004682  0.010300  0.004682  \n",
       "3  0.031092  0.019523  0.019523  0.016992  0.016269  0.014100  0.015907  \n",
       "4  0.024038  0.010989  0.026099  0.018544  0.014423  0.015797  0.019231  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "beafc5d0-9aa0-4ec0-9580-1b8172f23afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', ConfounderPCA()),\n",
       "                ('rf',\n",
       "                 BalancedRandomForestClassifier(max_depth=10, max_features=2,\n",
       "                                                min_samples_leaf=2,\n",
       "                                                min_samples_split=20,\n",
       "                                                n_estimators=500, n_jobs=-1,\n",
       "                                                random_state=42))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), # !! Absolutely necessary before PCA\n",
    "    (\"pca\", ConfounderPCA()), \n",
    "    (\"rf\", BalancedRandomForestClassifier(\n",
    "        max_depth=10,\n",
    "        max_features=2,\n",
    "        min_samples_leaf=2, \n",
    "        min_samples_split=20, \n",
    "        n_estimators=500,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e0fe110-6ebe-4cb9-ae4c-412d518cd6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.058677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.029464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.028224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.026352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.025785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.025348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.024915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.023124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.022813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score\n",
       "0   0.058677\n",
       "1   0.044785\n",
       "8   0.029464\n",
       "3   0.028224\n",
       "10  0.026352\n",
       "25  0.025785\n",
       "13  0.025348\n",
       "20  0.024915\n",
       "23  0.023124\n",
       "17  0.022813"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df = pd.DataFrame(rf_clf[\"rf\"].feature_importances_, columns=[\"Score\"])\n",
    "top_features_rf = importance_df.sort_values(by=\"Score\", key=abs, ascending=False).index.to_numpy()\n",
    "importance_df.sort_values(by=\"Score\", key=abs, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6763c7ab-624a-438b-ac64-bc8f519c300f",
   "metadata": {},
   "source": [
    "### [Permutation Feature Importance](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-importance)  \n",
    "\n",
    "> The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. _Scikit-Learn 4.2_\n",
    "\n",
    "> **Warning:** Features that are deemed of low importance for a bad model (low cross-validation score) could be very important for a good model. Therefore it is always important to evaluate the predictive power of a model using a held-out set (or better with cross-validation) prior to computing importances. Permutation importance **does not** reflect to the **intrinsic predictive value** of a feature by itself but how **important** this feature is **for a particular model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a37928bf-c8a0-4a33-82ee-5560ecf46601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e1f196e-a678-4ffd-b2d8-c57f8166b524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('lr',\n",
       "                 LogisticRegression(class_weight='balanced', max_iter=500))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    (\"lr\", LogisticRegression(max_iter=500, class_weight='balanced')),\n",
    "])\n",
    "\n",
    "lr_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0fabc1ec-f3ce-4137-bd9f-26a24487b933",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_lr = permutation_importance(lr_clf, X_train, y_train, n_repeats=10, random_state=0, scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "caa08706-ab11-4610-981a-29558e42a8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.102606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.093771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.092988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.077041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.074020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.072187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.071690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.071466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.069920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.069783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score\n",
       "79  0.102606\n",
       "66  0.093771\n",
       "3   0.092988\n",
       "71  0.077041\n",
       "28  0.074020\n",
       "31  0.072187\n",
       "77  0.071690\n",
       "32  0.071466\n",
       "50  0.069920\n",
       "46  0.069783"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df = pd.DataFrame(importance_lr.importances_mean, columns=[\"Score\"])\n",
    "top_features_lr_permutation = importance_df.sort_values(by=\"Score\", key=abs, ascending=False).index.to_numpy()\n",
    "importance_df.sort_values(by=\"Score\", key=abs, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "473c7f9e-5bc8-498a-916f-feae8689187d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()), ('pca', ConfounderPCA()),\n",
       "                ('rf',\n",
       "                 BalancedRandomForestClassifier(max_depth=10, max_features=2,\n",
       "                                                min_samples_leaf=2,\n",
       "                                                min_samples_split=20,\n",
       "                                                n_estimators=500, n_jobs=-1,\n",
       "                                                random_state=42))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), # !! Absolutely necessary before PCA\n",
    "    (\"pca\", ConfounderPCA()), \n",
    "    (\"rf\", BalancedRandomForestClassifier(\n",
    "        max_depth=10,\n",
    "        max_features=2,\n",
    "        min_samples_leaf=2, \n",
    "        min_samples_split=20, \n",
    "        n_estimators=500,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad62cdfa-a8ff-4f76-b28a-16e3a9c49f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_rf = permutation_importance(rf_clf, X_train, y_train, n_repeats=10, random_state=0, scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a13306eb-77f9-4ad9-9d97-5cf348b208d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.066150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.066024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.064253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.064115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.063890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score\n",
       "24  0.066150\n",
       "81  0.066024\n",
       "72  0.064253\n",
       "70  0.064115\n",
       "31  0.063890"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df = pd.DataFrame(importance_rf.importances_mean, columns=[\"Score\"])\n",
    "top_features_rf_permutation = importance_df.sort_values(by=\"Score\", key=abs, ascending=False).index.to_numpy()\n",
    "importance_df.sort_values(by=\"Score\", key=abs, ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "741f7751-d3da-4233-845c-57b4553ed4e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.015861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.029555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.043898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.062570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.060785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.036143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.066024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.058013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score\n",
       "0   0.007116\n",
       "1   0.015861\n",
       "2   0.029555\n",
       "3   0.016132\n",
       "4   0.043898\n",
       "..       ...\n",
       "78  0.062570\n",
       "79  0.060785\n",
       "80  0.036143\n",
       "81  0.066024\n",
       "82  0.058013\n",
       "\n",
       "[83 rows x 1 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "473dff9e-488c-46b9-ad3b-3a3007c3c3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('gnb', GaussianNB())])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb_clf = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    # (\"pca\", ConfounderPCA()), \n",
    "    (\"gnb\", GaussianNB()),\n",
    "])\n",
    "\n",
    "gnb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c08ec1e3-8d28-4409-95eb-332bca32c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_gnb = permutation_importance(gnb_clf, X_train, y_train, n_repeats=10, random_state=0, scoring='roc_auc', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4899c9c-f715-443c-986d-20dc8b8181ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.036090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.012107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.007616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.007154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.001074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.001017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score\n",
       "1   0.036090\n",
       "0   0.019175\n",
       "2   0.012107\n",
       "18  0.007616\n",
       "16  0.007154\n",
       "9   0.001076\n",
       "8  -0.001074\n",
       "6   0.001055\n",
       "15  0.001017\n",
       "10  0.000886"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df = pd.DataFrame(importance_gnb.importances_mean, columns=[\"Score\"])\n",
    "top_features_gnb = importance_df.sort_values(by=\"Score\", key=abs, ascending=False).index.to_numpy()\n",
    "importance_df.sort_values(by=\"Score\", key=abs, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cdf684bb-f776-4d9b-a302-6452503db1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svc',\n",
       "                 SVC(class_weight='balanced', probability=True, random_state=0,\n",
       "                     shrinking=False))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    # (\"pca\", ConfounderPCA()),\n",
    "    (\"svc\", SVC(\n",
    "        C=1.0, shrinking=False,\n",
    "        class_weight='balanced',\n",
    "        kernel=\"rbf\", \n",
    "        probability=True,\n",
    "        random_state=0,\n",
    "    ))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5122ddad-31fb-4b1a-baf5-60bb87e7992d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svc',\n",
       "                 SVC(class_weight='balanced', probability=True, random_state=0,\n",
       "                     shrinking=False))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "acc4b73f-898d-4147-8d5d-b28990bda0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:775: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return cache[method]\n",
      "KeyError: 'decision_function'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 355, in _score\n",
      "    y_pred = method_caller(clf, \"decision_function\", X)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    result = getattr(estimator, method)(*args, **kwargs)\n",
      "AttributeError: 'VotingClassifier' object has no attribute 'decision_function'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return cache[method]\n",
      "KeyError: 'predict_proba'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 369, in _score\n",
      "    y_pred = method_caller(clf, \"predict_proba\", X)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    result = getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 109, in __get__\n",
      "    if not self.check(obj):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\", line 362, in _check_voting\n",
      "    f\"predict_proba is not available when voting={repr(self.voting)}\"\n",
      "AttributeError: predict_proba is not available when voting='hard'\n",
      "\n",
      "  UserWarning,\n",
      "/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:775: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return cache[method]\n",
      "KeyError: 'decision_function'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 355, in _score\n",
      "    y_pred = method_caller(clf, \"decision_function\", X)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    result = getattr(estimator, method)(*args, **kwargs)\n",
      "AttributeError: 'VotingClassifier' object has no attribute 'decision_function'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return cache[method]\n",
      "KeyError: 'predict_proba'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 369, in _score\n",
      "    y_pred = method_caller(clf, \"predict_proba\", X)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    result = getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 109, in __get__\n",
      "    if not self.check(obj):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\", line 362, in _check_voting\n",
      "    f\"predict_proba is not available when voting={repr(self.voting)}\"\n",
      "AttributeError: predict_proba is not available when voting='hard'\n",
      "\n",
      "  UserWarning,\n",
      "/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:775: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return cache[method]\n",
      "KeyError: 'decision_function'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 355, in _score\n",
      "    y_pred = method_caller(clf, \"decision_function\", X)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    result = getattr(estimator, method)(*args, **kwargs)\n",
      "AttributeError: 'VotingClassifier' object has no attribute 'decision_function'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 71, in _cached_call\n",
      "    return cache[method]\n",
      "KeyError: 'predict_proba'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 762, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 369, in _score\n",
      "    y_pred = method_caller(clf, \"predict_proba\", X)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/metrics/_scorer.py\", line 73, in _cached_call\n",
      "    result = getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 109, in __get__\n",
      "    if not self.check(obj):\n",
      "  File \"/Applications/anaconda3/envs/ml_venv/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\", line 362, in _check_voting\n",
      "    f\"predict_proba is not available when voting={repr(self.voting)}\"\n",
      "AttributeError: predict_proba is not available when voting='hard'\n",
      "\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "importance_svm = permutation_importance(svm_clf, X_train, y_train, n_repeats=10, random_state=0, scoring='roc_auc', n_jobs=-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "48a577f1-8121-49f3-a0be-cb33d2a81b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.012117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.011668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.010164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.009802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.009672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.009495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.009184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.008640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.008560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Score\n",
       "0   0.026207\n",
       "31  0.012117\n",
       "64  0.011668\n",
       "13  0.010164\n",
       "58  0.009802\n",
       "75  0.009672\n",
       "44  0.009495\n",
       "79  0.009184\n",
       "63  0.008640\n",
       "80  0.008560"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df = pd.DataFrame(importance_svm.importances_mean, columns=[\"Score\"])\n",
    "top_features_svm = importance_df.sort_values(by=\"Score\", key=abs, ascending=False).index.to_numpy()\n",
    "importance_df.sort_values(by=\"Score\", key=abs, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec766530-8957-4600-95e5-69aaac1fbccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR highest scoring features:  [79 66  3 45 77 71 46 50 31 58 32 28 40 30 29 56 80 78 82 22]\n",
      "RF highest scoring features:  [ 0  1  8  3 10 25 13 20 23 17  4 30 19  5  7 46 36 41 18 24]\n",
      "GNB highest scoring features: [ 1  0  2 18 16  9  8  6 15 10 14  5 11 19  3  4 12  7 17 34]\n",
      "SVM highest scoring features: [ 0 31 64 13 58 75 44 79 63 80 65 14 12  4 82 78 62 22  1  9]\n"
     ]
    }
   ],
   "source": [
    "n = 20\n",
    "print(\"LR highest scoring features: \", top_features_lr[:n])\n",
    "print(\"RF highest scoring features: \", top_features_rf[:n]) # Uses PCA\n",
    "print(\"GNB highest scoring features:\", top_features_gnb[:n])\n",
    "print(\"SVM highest scoring features:\", top_features_svm[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fcc73-635b-4d27-9cef-008266cf4a8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3 - Cross-Validating the [Voting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)\n",
    "\n",
    "How did I determine the steps in each model's pipeline?\n",
    "- Random Forest: scaling is necessary for PCA, PCA improves random forest generalizatin (cit. Gèron) **should i opt not to reduce dimensionality?**\n",
    "- Logistic Regression: scaling improves model convergence\n",
    "- Gaussian Naive Bayes: does not seem to need either scaling or PCA\n",
    "- Support Vector Machine: SVM's are sensitive to feature scales\n",
    "\n",
    "And the parameters? Grid-search cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73c5df27-1756-4281-9e6d-1b4ac6baadcc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nan' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tr/8cjwzs2s60z4cw56zq2ptky80000gn/T/ipykernel_78101/1072024505.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0msvm_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0msv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msv_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mhv_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhv_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nan' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "rf_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), # !! Absolutely necessary before PCA\n",
    "    (\"pca\", ConfounderPCA()), \n",
    "    (\"rf\", BalancedRandomForestClassifier(\n",
    "        max_depth=10,\n",
    "        max_features=2,\n",
    "        min_samples_leaf=2, \n",
    "        min_samples_split=20, \n",
    "        n_estimators=500,\n",
    "        random_state=42, n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "lr_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()), # Improves convergence\n",
    "    # (\"pca\", ConfounderPCA()), \n",
    "    (\"lr\", LogisticRegression(max_iter=500, class_weight='balanced')),\n",
    "])\n",
    "\n",
    "gnb_clf = Pipeline([\n",
    "    # ('scaler', StandardScaler()),\n",
    "    # (\"pca\", ConfounderPCA()), \n",
    "    (\"gnb\", GaussianNB()),\n",
    "])\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    # (\"pca\", ConfounderPCA()),\n",
    "    (\"svc\", SVC(\n",
    "        C=1.0, shrinking=False,\n",
    "        class_weight='balanced',\n",
    "        kernel=\"rbf\", \n",
    "        probability=True,\n",
    "        random_state=0,\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "# Zero R Classifier as tiebreaker, from baseestimator api\n",
    "\n",
    "# Voting Classifier\n",
    "models = [(\"LR\", lr_clf), \n",
    "          (\"RF\",rf_clf),\n",
    "          (\"GNB\", gnb_clf), \n",
    "          (\"SVM\", svm_clf)\n",
    "         ]\n",
    "\n",
    "sv_clf = VotingClassifier(estimators=models, voting='soft', n_jobs=-1)\n",
    "hv_clf = VotingClassifier(estimators=models, voting='hard', n_jobs=-1)\n",
    "\n",
    "# Define metrics\n",
    "my_metrics = {\n",
    "    \"ROC_AUC\": \"roc_auc\",\n",
    "    \"AUPRC\": make_scorer(auprc, needs_proba=True,),  # custom: area under precision-recall-curve\n",
    "    \"Precision\": \"precision\",\n",
    "    \"Recall\": \"recall\",\n",
    "    \"f1-score\": \"f1\",\n",
    "}\n",
    "\n",
    "# K-fold cross-validate each model \n",
    "cv = 3\n",
    "lr_scores = cross_validate(lr_clf, X_train, y_train, cv=cv, scoring=my_metrics, n_jobs=-1)\n",
    "rf_scores = cross_validate(rf_clf, X_train, y_train, cv=cv, scoring=my_metrics, n_jobs=-1)\n",
    "gnb_scores = cross_validate(gnb_clf, X_train, y_train, cv=cv, scoring=my_metrics, n_jobs=-1)\n",
    "svm_scores = cross_validate(svm_clf, X_train, y_train, cv=cv, scoring=my_metrics, n_jobs=-1)\n",
    "sv_scores = cross_validate(sv_clf, X_train, y_train, cv=cv, scoring=my_metrics, error_score='raise', n_jobs=-1)\n",
    "hv_scores = cross_validate(hv_clf, X_train, y_train, cv=cv, scoring=my_metrics, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "935247eb-b721-495c-8ebe-507522dc6527",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df = pd.DataFrame(lr_scores)\n",
    "rf_df = pd.DataFrame(rf_scores)\n",
    "gnb_df = pd.DataFrame(gnb_scores)\n",
    "svm_df = pd.DataFrame(svm_scores)\n",
    "sv_df = pd.DataFrame(sv_scores)\n",
    "hv_df = pd.DataFrame(hv_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e1893052-06ef-49cb-8776-467b5db2ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_df.to_csv(\"./results/lr_scores10.csv\")\n",
    "rf_df.to_csv(\"./results/rf_scores10.csv\")\n",
    "gnb_df.to_csv(\"./results/gnb_scores10.csv\")\n",
    "svm_df.to_csv(\"./results/svm_scores10.csv\")\n",
    "sv_df.to_csv(\"./results/sv_scores10.csv\")\n",
    "hv_df.to_csv(\"./results/hv_scores10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "495bd3c3-e56a-4220-aed8-9d597e9cd506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogReg (Mean)</th>\n",
       "      <th>LogReg (SEM)</th>\n",
       "      <th>RF (Mean)</th>\n",
       "      <th>RF (SEM)</th>\n",
       "      <th>GNB (Mean)</th>\n",
       "      <th>GNB (SEM)</th>\n",
       "      <th>SVM (Mean)</th>\n",
       "      <th>SVM (SEM)</th>\n",
       "      <th>SoftVoting (Mean)</th>\n",
       "      <th>SoftVoting (SEM)</th>\n",
       "      <th>HardVoting (Mean)</th>\n",
       "      <th>HardVoting (SEM)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>0.532006</td>\n",
       "      <td>0.058210</td>\n",
       "      <td>7.150192</td>\n",
       "      <td>0.105805</td>\n",
       "      <td>0.032104</td>\n",
       "      <td>0.003390</td>\n",
       "      <td>102.274675</td>\n",
       "      <td>0.199837</td>\n",
       "      <td>124.882372</td>\n",
       "      <td>0.918890</td>\n",
       "      <td>83.322045</td>\n",
       "      <td>0.479760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.017182</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.832325</td>\n",
       "      <td>0.029453</td>\n",
       "      <td>0.031692</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>48.391898</td>\n",
       "      <td>0.940512</td>\n",
       "      <td>31.783226</td>\n",
       "      <td>0.355757</td>\n",
       "      <td>0.007477</td>\n",
       "      <td>0.005448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_ROC_AUC</th>\n",
       "      <td>0.664236</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.665953</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.633566</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.666808</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.673307</td>\n",
       "      <td>0.005965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_AUPRC</th>\n",
       "      <td>0.814095</td>\n",
       "      <td>0.039436</td>\n",
       "      <td>0.360528</td>\n",
       "      <td>0.014278</td>\n",
       "      <td>0.644872</td>\n",
       "      <td>0.008046</td>\n",
       "      <td>0.465879</td>\n",
       "      <td>0.018535</td>\n",
       "      <td>0.528159</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_Precision</th>\n",
       "      <td>0.316219</td>\n",
       "      <td>0.005078</td>\n",
       "      <td>0.316593</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>0.411641</td>\n",
       "      <td>0.014239</td>\n",
       "      <td>0.317369</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.429747</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_Recall</th>\n",
       "      <td>0.626861</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.642662</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.151322</td>\n",
       "      <td>0.009251</td>\n",
       "      <td>0.612276</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.181708</td>\n",
       "      <td>0.016352</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_f1-score</th>\n",
       "      <td>0.420367</td>\n",
       "      <td>0.006114</td>\n",
       "      <td>0.424189</td>\n",
       "      <td>0.003485</td>\n",
       "      <td>0.220495</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.418033</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>0.254003</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                LogReg (Mean)  LogReg (SEM)  RF (Mean)  RF (SEM)  GNB (Mean)  \\\n",
       "fit_time             0.532006      0.058210   7.150192  0.105805    0.032104   \n",
       "score_time           0.017182      0.001824   0.832325  0.029453    0.031692   \n",
       "test_ROC_AUC         0.664236      0.005098   0.665953  0.003536    0.633566   \n",
       "test_AUPRC           0.814095      0.039436   0.360528  0.014278    0.644872   \n",
       "test_Precision       0.316219      0.005078   0.316593  0.001779    0.411641   \n",
       "test_Recall          0.626861      0.008039   0.642662  0.009311    0.151322   \n",
       "test_f1-score        0.420367      0.006114   0.424189  0.003485    0.220495   \n",
       "\n",
       "                GNB (SEM)  SVM (Mean)  SVM (SEM)  SoftVoting (Mean)  \\\n",
       "fit_time         0.003390  102.274675   0.199837         124.882372   \n",
       "score_time       0.003426   48.391898   0.940512          31.783226   \n",
       "test_ROC_AUC     0.006674    0.666808   0.001256           0.673307   \n",
       "test_AUPRC       0.008046    0.465879   0.018535           0.528159   \n",
       "test_Precision   0.014239    0.317369   0.002983           0.429747   \n",
       "test_Recall      0.009251    0.612276   0.002373           0.181708   \n",
       "test_f1-score    0.007508    0.418033   0.002860           0.254003   \n",
       "\n",
       "                SoftVoting (SEM)  HardVoting (Mean)  HardVoting (SEM)  \n",
       "fit_time                0.918890          83.322045          0.479760  \n",
       "score_time              0.355757           0.007477          0.005448  \n",
       "test_ROC_AUC            0.005965                NaN               NaN  \n",
       "test_AUPRC              0.010998                NaN               NaN  \n",
       "test_Precision          0.011440                NaN               NaN  \n",
       "test_Recall             0.016352                NaN               NaN  \n",
       "test_f1-score           0.013743                NaN               NaN  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results_df[\"LogReg (Mean)\"] = lr_df.mean()\n",
    "results_df[\"LogReg (SEM)\"] = lr_df.sem()\n",
    "\n",
    "results_df[\"RF (Mean)\"] = rf_df.mean()\n",
    "results_df[\"RF (SEM)\"] = rf_df.sem()\n",
    "\n",
    "results_df[\"GNB (Mean)\"] = gnb_df.mean()\n",
    "results_df[\"GNB (SEM)\"] = gnb_df.sem()\n",
    "\n",
    "results_df[\"SVM (Mean)\"] = svm_df.mean()\n",
    "results_df[\"SVM (SEM)\"] = svm_df.sem()\n",
    "\n",
    "results_df[\"SoftVoting (Mean)\"] = sv_df.mean()\n",
    "results_df[\"SoftVoting (SEM)\"] = sv_df.sem()\n",
    "\n",
    "results_df[\"HardVoting (Mean)\"] = hv_df.mean()\n",
    "results_df[\"HardVoting (SEM)\"] = hv_df.sem()\n",
    "\n",
    "results_df.to_csv(\"./results/cv_results.csv\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9587f392-288c-4f08-a02b-31b401285dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=-1.0661729850348076, pvalue=0.3464223785692395)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "print(ttest_ind(svm_df.test_ROC_AUC, sv_df.test_ROC_AUC))\n",
    "# print(ttest_ind(svm_df.test_ROC_AUC, hv_df.test_ROC_AUC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5593676-3287-4534-9510-8f1297152944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b33e2e3e-ea5d-498e-aef3-ae06018b68fb",
   "metadata": {},
   "source": [
    "[Nested CV](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html) for all models\n",
    "\n",
    "feature importance bene e ordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd122631-4338-4289-b00c-b93526266eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_venv]",
   "language": "python",
   "name": "conda-env-ml_venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
